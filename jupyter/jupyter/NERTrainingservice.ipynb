{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to Aspect AI Model Training Service Notebook!\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.basicConfig(filename='NER_Training_Service.log', filemode='w',format='%(asctime)s - %(message)s', level=logging.INFO)\n",
    "logging.info(\"started model training\")\n",
    "print(\"Welcome to Aspect AI Model Training Service Notebook!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training Library Imports and Environment Setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.8) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os import path\n",
    "import sys\n",
    "import shutil\n",
    "import time\n",
    "from datetime import datetime\n",
    "import ast\n",
    "import json\n",
    "import re\n",
    "import copy\n",
    "import glob\n",
    "import requests\n",
    "import urllib\n",
    "import hashlib\n",
    "import multiprocessing\n",
    "import subprocess\n",
    "import threading\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', '.*type is deprecated.*', )\n",
    "\n",
    "def install(package):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "localhost\n"
     ]
    }
   ],
   "source": [
    "# added to use static image in model deployment\n",
    "os.environ['PYPI_PACKAGE_REPO'] = 'localhost'\n",
    "print(os.environ['PYPI_PACKAGE_REPO'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in mosaic-ai -  ['.bash_logout', '.bashrc', '.profile', '.cache', '.mosaic.ai', '.ipython', '.config', 'mosaic_nb_extension', 'mc']\n"
     ]
    }
   ],
   "source": [
    "# Appending required paths\n",
    "# logging.basicConfig(filename='NER_Training_Service.log', filemode='w',format='%(asctime)s - %(message)s', level=logging.INFO)\n",
    "\n",
    "sys.path.insert(0,'/notebooks/notebooks/EEF_NER_LSTM_Training')\n",
    "sys.path.insert(0,'/notebooks/notebooks/nltk_data')\n",
    "sys.path.insert(0,'/home/mosaic-ai/.local/bin')\n",
    "\n",
    "logging.info(sys.path)\n",
    "print(\"Files in mosaic-ai - \",os.listdir('/home/mosaic-ai/'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pdf2image'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-0ca38f0e384e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mEEF_NER_LSTM_Training\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mservices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_classify\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpreprocessing\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mimg_prepro\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mEEF_NER_LSTM_Training\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mservices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobject_detection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlabel_map_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mEEF_NER_LSTM_Training\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mservices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobject_detection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minference\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0minfer_zoning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;31m# Loading application configs using app utility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mCONFIG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapp_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_system_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/notebooks/notebooks/EEF_NER_LSTM_Training/services/ai/object_detection/inference.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# added for pdf to image conversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpdf2image\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconvert_from_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\"\"\"Inference utilities for Object detection model.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pdf2image'"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "import pdf2image\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from collections import OrderedDict\n",
    "from flask import request\n",
    "import nltk\n",
    "from nltk.chunk import conlltags2tree\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import Progbar\n",
    "from keras.models import load_model as keras_load_model\n",
    "import tensorflow as tf\n",
    "from utility import app_constants, logger_util, http_util\n",
    "from utility import utility_application as app_util\n",
    "from mosaicml import *\n",
    "from mosaicml.constants import MLModelFlavours\n",
    "from mosaicml import list_models, describe_model, register_model, load_model, deploy_model, scoring_func, generate_schema, stop_model, update_metadata_info, add_version, update_existing_model\n",
    "from EEF_NER_LSTM_Training.cloud_utility import download_file_object_from_cloud_storage, upload_file_obj_to_cloud_storage, get_all_files_from_cloud_dir\n",
    "from EEF_NER_LSTM_Training.services.ai.image_classify import preprocessing as img_prepro\n",
    "from EEF_NER_LSTM_Training.services.ai.object_detection.utils import label_map_util\n",
    "from EEF_NER_LSTM_Training.services.ai.object_detection import inference as infer_zoning\n",
    "# Loading application configs using app utility\n",
    "CONFIG = app_util.get_system_config()[1]\n",
    "OnPremiseFlag = CONFIG['OnPremiseFlag']\n",
    "CONFIG_Zonning = app_util.get_system_config()[4]\n",
    "image_config = app_util.get_system_config()[3]\n",
    "application_config = app_util.get_system_config()[1]\n",
    "graph = tf.get_default_graph()\n",
    "logging.info(\"Loaded the required packages and tf graph\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@scoring_func\n",
    "def dummy_scoring_function(model,request):\n",
    "    \"\"\"\n",
    "    dummy scoring function\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_model(created_by, files, parent_id, name, solution_id,\n",
    "              model_type=None, current_version = 1, language = 'english',\n",
    "              mode_type = \"\", description = \"\", epochs = '',ail_model_id= '',ail_version_id = '',flavour=''):\n",
    "    \n",
    "    \"\"\"\n",
    "    Add model to the database and return the new model id.\n",
    "\n",
    "    Returns :\n",
    "        model_id = Get the status of the particular model id\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(parent_id, int) != True:\n",
    "        current_version += 1\n",
    "\n",
    "    if epochs == '':\n",
    "        epochs = int(CONFIG[\"EPOCHS\"])\n",
    "\n",
    "    created_on = str(datetime.now())\n",
    "    last_updated_on = str(created_on)\n",
    "    status = app_constants.STATUS_INPROGRESS\n",
    "    environment = CONFIG['AI_ENV']\n",
    "    solution_dtl_req_url = CONFIG[\"AI_SOLUTION_DTL_URL\"]\n",
    "    OI_RescaleImgSize= CONFIG_Zonning['MODEL_TRAINING_IMAGE_SIZE']\n",
    "    parameter = {}\n",
    "    parameter[\"solution_id\"] = solution_id\n",
    "    get_request = http_util.post(solution_dtl_req_url, parameter, created_by)\n",
    "    print(\"get soln details input \",solution_dtl_req_url, parameter, created_by)\n",
    "    if get_request.status_code != app_constants.RESPONSE_OK:\n",
    "        print(\"Failed to get solution details from Aspect \",app_constants.FAILED)\n",
    "        return app_constants.FAILED\n",
    "\n",
    "    solution_dtl = get_request.json()\n",
    "    solution_name = str(solution_dtl.get('name'))\n",
    "    \n",
    "    metadata_info = {'name':name, 'created_by':created_by, 'created_on':created_on, 'last_updated_on':last_updated_on,\n",
    "                     'parent_id':parent_id, 'status':status, 'metrics':None, 'output_files':None, 'epochs':epochs,\n",
    "                     'progress':0, 'labels':'', 'environment':environment, 'is_active':1, 'current_version':current_version,\n",
    "                     'model_type':model_type, 'mode_type':mode_type, 'language':language, 'description':description, 'files':files,\n",
    "                     'ail_model_id':ail_model_id, 'ail_version_id':ail_version_id, 'solution_id':solution_id,\n",
    "                     'solution_name':solution_name, 'OI_ImageRescaleSize':OI_RescaleImgSize,\"img_rescale_flag\":False}\n",
    "\n",
    "    print(\"Model metadata info - \",metadata_info)\n",
    "    \n",
    "    model = None\n",
    "    flavour = MLModelFlavours.sklearn\n",
    "    print('Flavour of model - ',flavour)\n",
    "\n",
    "    if current_version==1:\n",
    "        print(\"current_version - 1\")\n",
    "        register_model_patience = 0\n",
    "        while register_model_patience < 10:\n",
    "            try:\n",
    "                RegisterModel = register_model(model, name=name, flavour=flavour,  description=description,\n",
    "                                               scoring_func=dummy_scoring_function,metadata_info=metadata_info,\n",
    "                                               pretty_output=False)\n",
    "                time.sleep(10)\n",
    "                print(\"Details of new model registred\")\n",
    "                break\n",
    "            except BaseException as e:\n",
    "                print(\"Failed to register the model with error \" , e )\n",
    "                register_model_patience += 1\n",
    "                print(\"Register_model_patience \",register_model_patience)\n",
    "                time.sleep(10)\n",
    "                pass            \n",
    "\n",
    "    else:\n",
    "        describe_model_patience = 0\n",
    "        while describe_model_patience < 10:\n",
    "            try:\n",
    "                mm = describe_model(parent_id.split(\"__\")[0])\n",
    "                time.sleep(10)\n",
    "                print(\"Parent model details - \", mm)\n",
    "                break\n",
    "            except BaseException as e:\n",
    "                print(\"Failed to get describe model details with error \", e)\n",
    "                describe_model_patience += 1\n",
    "                print(\"Describe_model_patience - \", describe_model_patience)\n",
    "                time.sleep(10)\n",
    "                pass             \n",
    "\n",
    "        mm['flavour'] = flavour\n",
    "\n",
    "        register_model_patience = 0\n",
    "        while register_model_patience < 10:\n",
    "            try:\n",
    "                RegisterModel = add_version(mm, model,scoring_func = \n",
    "                                            dummy_scoring_function,flavour=flavour,init_script=mm['versions'][0]\n",
    "                                            ['init_script'], input_type=\"json\", metadata_info=metadata_info)\n",
    "                time.sleep(10)\n",
    "                print(\"Details on add model version - \",RegisterModel)\n",
    "                break\n",
    "            except BaseException as e:\n",
    "                print(\"Failed to add version to the model with error \", e)\n",
    "                register_model_patience += 1\n",
    "                print(\"Register_model_patience \",register_model_patience)\n",
    "                time.sleep(10)\n",
    "                pass\n",
    "\n",
    "    ver = sorted(RegisterModel['versions'], key = lambda i: i['created_on'])\n",
    "    latest_ver_id = ver[len(ver)-1]['id']\n",
    "\n",
    "    model_id = RegisterModel['id']+'__'+latest_ver_id\n",
    "\n",
    "    print(\"New model id - \",model_id)\n",
    "\n",
    "    return model_id            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'language'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-1de57da65707>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# For Postmen execusion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlanguage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"language\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mcreated_by\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"created_by\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'files'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\"'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/os.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0;31m# raise KeyError with the original key value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecodevalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'language'"
     ]
    }
   ],
   "source": [
    "# For Postmen execusion\n",
    "language = os.environ[\"language\"]\n",
    "created_by = os.environ[\"created_by\"]\n",
    "files = os.environ['files']\n",
    "files = files.strip('\"')\n",
    "files = eval(files)\n",
    "\n",
    "try:\n",
    "    model_id = int(eval(os.environ['model_id']))\n",
    "except:\n",
    "    print(\"Failed to create int model id\")\n",
    "    model_id = os.environ['model_id']+\"__\"+os.environ['version_id']\n",
    "# Added for DC NER CC Retraining \n",
    "try:\n",
    "    label = os.environ[\"label\"]\n",
    "    print(\" Label recived for DC NER CC retraining flow - \", label)\n",
    "except:\n",
    "    label = None\n",
    "    print(\" Label not recived, set to None\")\n",
    "\n",
    "solution_id = str(os.environ['solution_id'])\n",
    "current_version = eval(os.environ['current_version'])\n",
    "name = os.environ['name']\n",
    "model_type = os.environ['model_type']\n",
    "mode_type = os.environ['mode_type']\n",
    "description =  os.environ['description']\n",
    "\n",
    "if ((len(eval(model_type)) == 1) and (eval(model_type)[0] == 'DC')):\n",
    "    epochs = 'NA'\n",
    "else:\n",
    "    epochs =  int(eval(os.environ['epochs']))    \n",
    "\n",
    "parent_id = model_id\n",
    "\n",
    "try:\n",
    "    level = os.environ['level']\n",
    "except:\n",
    "    print(\"level not defined so seting to document level\")\n",
    "    level = 'document'\n",
    "    \n",
    "try:\n",
    "    ail_model_id = model_id.split(\"__\")[0]\n",
    "    ail_version_id = model_id.split(\"__\")[1]\n",
    "except:\n",
    "    ail_model_id = model_id\n",
    "    ail_version_id = model_id\n",
    "\n",
    "try:\n",
    "    multiple_crop_zone = os.environ['multiple_crop_zone']\n",
    "    multiple_crop_zone = multiple_crop_zone.strip('\"')\n",
    "    multiple_crop_zone = eval(multiple_crop_zone)\n",
    "    print(\" multiple_crop_zone\", multiple_crop_zone)\n",
    "    if len(multiple_crop_zone)>0:\n",
    "        for i in multiple_crop_zone:\n",
    "            if i['multi_crop_flag'] ==True:\n",
    "                OI_MultiCropFlag = True\n",
    "            else:\n",
    "                i['multi_crop_flag']==False\n",
    "                OI_MultiCropFlag = False\n",
    "    print(\" Recived MultiCropFlag from UI\")\n",
    "except:\n",
    "    OI_MultiCropFlag = False\n",
    "    \n",
    "print(\"OI_MultiCropFlag \",OI_MultiCropFlag)    \n",
    "print(\"Importing param done.\")\n",
    "\n",
    "model = None\n",
    "\n",
    "if len(eval(model_type)) == 1:\n",
    "    new_model_id = add_model(created_by, files, parent_id, name, solution_id, eval(model_type)[0],int(current_version), language, mode_type, description, epochs,ail_model_id,ail_version_id,flavour=MLModelFlavours.sklearn)\n",
    "    print(\"New model id created for Non Ensemble model - \",new_model_id)\n",
    "elif len(eval(model_type)) == 2:\n",
    "    if 'DC' in model_type and 'NER' in model_type:\n",
    "        new_model_id = add_model(created_by, files, parent_id, name, solution_id, 'DC',int(current_version), language, mode_type, description, epochs,ail_model_id,ail_version_id,flavour=MLModelFlavours.sklearn)\n",
    "        print(\"New model id created for Ensemble DC model - \", new_model_id)\n",
    "    if \"IC\" in model_type and \"Object Identification\" in model_type:\n",
    "        new_model_id = add_model(created_by, files, parent_id, name,solution_id, 'IC',int(current_version), language, mode_type, description, epochs,ail_model_id,ail_version_id,flavour=MLModelFlavours.sklearn)\n",
    "        print(\"New model id created for Ensembl IC model\",new_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting only the application config utility\n",
    "from EEF_NER_LSTM_Training.utility import utility_application as app_util\n",
    "CONFIG = app_util.get_system_config()[1]\n",
    "OnPremiseFlag = CONFIG['OnPremiseFlag']\n",
    "nltk_supported_lang = CONFIG['nltk_supported_lang']\n",
    "latin_encoding_lang = CONFIG['latin_encoding_lang']\n",
    "utf_encoding_lang = CONFIG['utf_encoding_lang']\n",
    "OnPremiseFlag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # installing packages for mongolian language\n",
    "if language ==\"mongolian\":\n",
    "    try:\n",
    "        print(\" Installing mongolian language packages \")\n",
    "#         install(\"polyglot\")\n",
    "#         install(\"pyicu\")\n",
    "#         install(\"pycld2\")\n",
    "#         install(\"morfessor\")\n",
    "        import polyglot\n",
    "        print(\" Installation done for mongolian language packages \")\n",
    "    except:\n",
    "        pass\n",
    "        print(\"skiped mongolian package installation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('/notebooks/Train_models'):\n",
    "    os.makedirs('/notebooks/Train_models')\n",
    "nltk.data.path.append('/data/NER/nltk_data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " nltk_supported_lang  ['english', 'french', 'spanish', 'swedish', 'russian']\n",
      " latin_encoding_lang  ['english', 'french', 'spanish', 'swedish']\n",
      " utf_encoding_lang  ['mongolian', 'russian']\n",
      " IMG_SIZE from local config  (512, 512)\n",
      "  img_rescale_flag  False\n",
      "OnPremiseFlag  False\n",
      " nltk_supported_lang  ['english', 'french', 'spanish', 'swedish', 'russian']\n",
      " latin_encoding_lang  ['english', 'french', 'spanish', 'swedish']\n",
      " utf_encoding_lang  ['mongolian', 'russian']\n",
      " OI IMG_SIZE  (512, 512)\n",
      "OnPremiseFlag  False\n",
      " nltk_supported_lang  ['english', 'french', 'spanish', 'swedish', 'russian']\n",
      " latin_encoding_lang  ['english', 'french', 'spanish', 'swedish']\n",
      " utf_encoding_lang  ['mongolian', 'russian']\n",
      "OnPremiseFlag  False\n",
      " nltk_supported_lang  ['english', 'french', 'spanish', 'swedish', 'russian']\n",
      " latin_encoding_lang  ['english', 'french', 'spanish', 'swedish']\n",
      " utf_encoding_lang  ['mongolian', 'russian']\n",
      " OI IMG_SIZE  (512, 512)\n"
     ]
    }
   ],
   "source": [
    "#if OnPremiseFlag!=True:\n",
    "    #from EEF_NER_LSTM_Training.S3_module import conn ,bucket, s3, s3_resource\n",
    "from EEF_NER_LSTM_Training.OI_Progress_function import OI_progress\n",
    "from EEF_NER_LSTM_Training.OI_Rescale_Flag import get_OI_rescale_flag\n",
    "from EEF_NER_LSTM_Training.manager.model import ModelManager, initialize_model_pool\n",
    "from EEF_NER_LSTM_Training.services.ai.lstm_cnn.train import (\n",
    "    training_in_progress,\n",
    "    create_training_event)\n",
    "from EEF_NER_LSTM_Training.services.ai.lstm_cnn.content_classification import (\n",
    "    get_region_of_interest,\n",
    "    train_content_classification_model)\n",
    "from EEF_NER_LSTM_Training.manager.annotation import Annotation\n",
    "from EEF_NER_LSTM_Training.manager.mosaicml_model import update_mosaicml_metadata_info, get_mosaicml_metadata_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training Flow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train module started\n",
      " global new_model_id  1__1\n",
      "'language'\n",
      "out from TrainModel\n"
     ]
    }
   ],
   "source": [
    "# Complete Training Flow\n",
    "\"\"\"Routes for training as a service.\"\"\"\n",
    "\"\"\" This code is taken from service-model.py\"\"\"\n",
    "\n",
    "import ast\n",
    "LOGGER = logger_util.get_logger(\"Model Training Flow\")\n",
    "\n",
    "def train():\n",
    "    print(\"Train module started\")\n",
    "    global new_model_id, parent_id, mode_type, model_type, language, created_by, files, epochs, level, label, solution_id,name\n",
    "    print(\"Global new_model_id \",new_model_id)\n",
    "    zone_config = {}\n",
    "    try:\n",
    "        # Check whether model training already in progress.\n",
    "\n",
    "        if (isinstance(parent_id, int) != True) and (OnPremiseFlag != True):\n",
    "            basemodel_path = 'EEF_models/' + str(parent_id) + os.sep\n",
    "            if not os.path.exists(os.path.join(CONFIG['MODEL_BASE_LOCATION'], str(parent_id))):\n",
    "                os.makedirs(os.path.join(CONFIG['MODEL_BASE_LOCATION'], str(parent_id)))\n",
    "                print(\"Model Directory Created\")\n",
    "            else:\n",
    "                print(\"Model Directory Already Exist\")\n",
    "            \n",
    "            file_name_list = list()\n",
    "            cloud_file_path_list = get_all_files_from_cloud_dir(basemodel_path, CONFIG['STORAGE_AT'])\n",
    "            for cloud_file_path in cloud_file_path_list:\n",
    "                file_name_list.append(cloud_file_path.get('cloud_path'))\n",
    "\n",
    "            download_folder_name_connector_pvc = str(uuid.uuid1())\n",
    "            for file_name in file_name_list:\n",
    "                folder_path = download_file_object_from_cloud_storage(CONFIG['STORAGE_AT'], file_name,\n",
    "                                          download_folder_name_connector_pvc,\n",
    "                                          temp_folder=os.path.join(CONFIG['MODEL_BASE_LOCATION'], str(parent_id)))\n",
    "            temp_folder=os.path.join(CONFIG['MODEL_BASE_LOCATION'], str(parent_id))\n",
    "            dir_list = os.listdir(temp_folder)\n",
    "            print(\" list of parent models files \",dir_list)\n",
    "\n",
    "        elif (isinstance(parent_id, int) != True) and (OnPremiseFlag == True):\n",
    "            basemodel_path = CONFIG['MODEL_BASE_LOCATION'] +\"/\"+ str(parent_id)+'/'\n",
    "\n",
    "            print(\"PV model directory created for base model\")\n",
    "            pv_model_path = CONFIG['TRAINED_MODEL_BASE_PATH'] + str(parent_id) + '/'\n",
    "            print(\" files under pre train model \", pv_model_path , os.listdir(pv_model_path))\n",
    "            try:\n",
    "                print(\" files copying to notebooks pod \", basemodel_path)                \n",
    "                shutil.copytree(pv_model_path, basemodel_path)\n",
    "                print(\" files copied to Notebook POD\",os.listdir(basemodel_path))\n",
    "            except BaseException as e :\n",
    "                print(\"fail to copy model from data PV \",e)                \n",
    "                    \n",
    "        model_manager = ModelManager()\n",
    "        thread_start_flag = True\n",
    "        train_process = None\n",
    "        if len(eval(model_type)) == 1:\n",
    "            print(\"length of model_type = 1\")                                                    \n",
    "            print(\"new_model_id created\",new_model_id)\n",
    "            Ensemble_ModelDetails = []\n",
    "            annotation = Annotation(language)\n",
    "            model_id = new_model_id\n",
    "            \n",
    "            try:\n",
    "                print(\"Fetching details on Content Classification\")\n",
    "                CCFlag, file_data = annotation.Get_CC_Flag_Data(files, solution_id, model_id, created_by)\n",
    "                print(\"CCFlag value \",CCFlag)\n",
    "                if CCFlag == \"True\":\n",
    "                    train_content_classification_model(model_id, file_data)\n",
    "                    print(\"Content Classification Data fetched successfully\")    \n",
    "            except:\n",
    "                print(\"unable to fetch the Content Classification data, keeping CCFlag False\")\n",
    "                CCFlag = \"False\"\n",
    "                print(\"CCFlag value \",CCFlag)\n",
    "            \n",
    "            if eval(model_type)[0] == 'NER' and mode_type == 'mode_1':\n",
    "                print(\"Training NER/MODE-1 Model\")\n",
    "                \n",
    "                train_process = threading.Thread(\n",
    "                    target=ModelManager().train, args=[\n",
    "                        new_model_id, files, parent_id, created_by,label, solution_id, language, epochs\n",
    "                    ])\n",
    "            \n",
    "            elif eval(model_type)[0] == 'NER' and mode_type == 'mode_2':\n",
    "                print(\"Training NER/MODE-2 Model\")\n",
    "                train_process = threading.Thread(\n",
    "                    target=ModelManager().train_spacy_ner, args=[\n",
    "                        new_model_id, files, parent_id, created_by, solution_id, language, epochs\n",
    "                    ])\n",
    "                \n",
    "            elif eval(model_type)[0] == 'DC':\n",
    "                print(\"Training Document Classification Model\")\n",
    "                train_process = threading.Thread(\n",
    "                    target=ModelManager().train_document_classification, args=[\n",
    "                        new_model_id, files, parent_id, created_by, level, solution_id\n",
    "                    ])\n",
    "                \n",
    "            elif eval(model_type)[0] == 'IC':\n",
    "                print(\"Training Image Classification Model\")\n",
    "                input_img_type =  os.getenv('input_img_type','png')\n",
    "                train_process = threading.Thread(\n",
    "                    target=ModelManager().train_img_classify, args=[\n",
    "                        new_model_id, files, created_by, parent_id, solution_id, input_img_type, language, epochs\n",
    "                    ])\n",
    "                \n",
    "            elif eval(model_type)[0] == \"Object Identification\":\n",
    "                print(\"Training Object Identification Model\")\n",
    "                zone_config,status = annotation.get_zone_config(new_model_id,files,parent_id,solution_id, created_by)\n",
    "                print(\" zone_config for OI model \", zone_config)                \n",
    "                train_process = threading.Thread(\n",
    "                    target=ModelManager().train_zoning, args=[\n",
    "                        new_model_id, files,parent_id,solution_id,epochs, created_by\n",
    "                    ])\n",
    "        \n",
    "        elif len(eval(model_type)) == 2:\n",
    "            print(\"length of model_type = 2\")\n",
    "            if 'DC' in model_type and 'NER' in model_type:\n",
    "                #Saving model details for future refrence\n",
    "                Ensemble_ModelDetails = []\n",
    "                DC_details = {\"model_id\":new_model_id, 'name':name, \"files\":files, \"model_type\":'DC', \"CCFlag\":\"False\", \"epochs\":'NA'}\n",
    "                Ensemble_ModelDetails.append(DC_details.copy())\n",
    "                \n",
    "                annotation = Annotation(language)\n",
    "                result,result_status = annotation.get_doc_classify_training_data(new_model_id, files, solution_id, created_by,level)\n",
    "                print(\"Prepared document classification training data.\")             \n",
    "                \n",
    "                if result_status == app_constants.SUCCESS:\n",
    "                    train_process = threading.Thread(target=ModelManager().train_document_classification, args=[\n",
    "                        new_model_id, files, parent_id, created_by,level, solution_id\n",
    "                    ])\n",
    "                                \n",
    "                    training_data = result\n",
    "\n",
    "                    classification_labels = set(list(training_data['carrier_name']))\n",
    "                    print(\"Document classification labels\", classification_labels)\n",
    "                    classification_labels_files = training_data.drop(\"page_data\", axis=1)\n",
    "\n",
    "                    for label in classification_labels:\n",
    "                        print(\"Document class \", label)\n",
    "                        files_label_specific1 = list(classification_labels_files.loc[classification_labels_files['carrier_name']==str(label), 'file_id'])\n",
    "                        files_label_specific = []\n",
    "                        for i in files_label_specific1:\n",
    "                            files_label_specific.append(str(i))\n",
    "                        print(\"Class specific files \", files_label_specific)\n",
    "\n",
    "                        tm = int(round(time.time()))\n",
    "                        #name = 'DC_'+label+'_NER_'+str(tm)\n",
    "                        cl_mdl_name = name+'_Class_'+label+'_NER'\n",
    "                        print(\"Class Specific NER model \", cl_mdl_name)\n",
    "                        current_version = 1\n",
    "                        parent_id = -1\n",
    "                        ail_model_id = -1\n",
    "                        ail_version_id = -1\n",
    "                        mode_type = \"mode_1\"\n",
    "\n",
    "                        new_model_id = add_model(created_by, files_label_specific, parent_id, cl_mdl_name,solution_id, 'NER',\n",
    "                                                               int(current_version), language, mode_type, description, epochs,\n",
    "                                                               ail_model_id, ail_version_id, flavour=MLModelFlavours.keras)\n",
    "\n",
    "                        annotation = Annotation(language)\n",
    "                        model_id = new_model_id\n",
    "                        \n",
    "                        try:\n",
    "                            print(\"Fetch details of Content Classification\")\n",
    "                            CCFlag,file_data = annotation.Get_CC_Flag_Data_DC(files_label_specific, solution_id, model_id,label, created_by)\n",
    "                            print(\"CCFlag value \", CCFlag)\n",
    "                            if CCFlag == \"True\":\n",
    "                                train_content_classification_model(model_id, file_data)\n",
    "                                print(\"Content Classification Data fetched successfully \")\n",
    "                        except:\n",
    "                            print(\"unable to fetch the Content Classification data, keeping CCFlag False\")\n",
    "                            CCFlag = \"False\"\n",
    "                            print(\"CCFlag value \", CCFlag)\n",
    "\n",
    "                        NER_details = {\"model_id\":new_model_id, \"name\":cl_mdl_name, \"files\":files_label_specific, \"model_type\":'NER', \"CCFlag\":CCFlag, \"epochs\":epochs, \"language\":language}\n",
    "                        Ensemble_ModelDetails.append(NER_details.copy())\n",
    "\n",
    "                        DCNER_process = threading.Thread(target=ModelManager().train, args=[new_model_id, files_label_specific, parent_id, created_by,label, solution_id, language,epochs])\n",
    "                        DCNER_process.start()\n",
    "                        DCNER_process.join()\n",
    "                        print(\"Training NER follwed by DC Model\")\n",
    "                        model_path = os.path.join(CONFIG['MODEL_BASE_LOCATION'], new_model_id)\n",
    "                        valid_char_file_path = model_path+'/valid_char_set.pkl'\n",
    "                        print(\"valid_char_file_path \", valid_char_file_path)\n",
    "\n",
    "                        #if os.path.isfile(valid_char_file_path):\n",
    "                        #    print(\"Recived new files\")\n",
    "                        #    time.sleep(30)\n",
    "                        #else:\n",
    "                        #    raise ValueError(\"%s isn't a file!\" % file_path)                        \n",
    "                                        \n",
    "                elif result_status == app_constants.FAILED:\n",
    "                    print(\"Failed to get the AI Tutor data for document classification. \")\n",
    "                    #print(\"Clearing training event\")\n",
    "                    #if clear_training_event():\n",
    "                    #    print(\"Cleared training event\")\n",
    "                    update_mosaicml_metadata_info(new_model_id,data={'status':_app_constants.STATUS_FAILED})\n",
    "                    return app_constants.FAILED\n",
    "\n",
    "            if \"IC\" in model_type and \"Object Identification\" in model_type:\n",
    "                input_img_type =  os.getenv('input_img_type','png')\n",
    "                CCFlag = \"False\"\n",
    "                #Saving model details for future refrence\n",
    "                Ensemble_ModelDetails = []\n",
    "                IC_details = {\"model_id\":new_model_id, 'name':name, \"files\":files, \"model_type\":'IC', \"CCFlag\":\"False\", \"epochs\":epochs}\n",
    "                Ensemble_ModelDetails.append(IC_details.copy())\n",
    "                \n",
    "                annotation = Annotation(language)\n",
    "                result,result_status = annotation.get_img_classify_training_data(new_model_id, files, solution_id, input_img_type, created_by)\n",
    "                print(\"Prepared the image classification training data.\")\n",
    "\n",
    "                if result_status == app_constants.SUCCESS:\n",
    "                    train_process = threading.Thread(target=ModelManager().train_img_classify, args=[\n",
    "                        new_model_id, files, created_by, parent_id, solution_id, input_img_type, language, epochs\n",
    "                    ])\n",
    "                    \n",
    "                    file_class = list(set([i.split('/')[-2] for i in result['file_paths']]))\n",
    "                    file_class_list = [i.split('/')[-2] for i in result['file_paths']]\n",
    "                    \n",
    "                    \n",
    "                    print(\"Image classification labels \",file_class)\n",
    "                    \n",
    "                    for label in file_class:\n",
    "                        file_nm = \"oi_progress_\"+str(label)+\".log\"\n",
    "                        file_nm_path = os.path.join('/home/mosaic-ai/'+file_nm)\n",
    "                        root_logger = logging.getLogger()\n",
    "                        new_handler = logging.FileHandler(file_nm)\n",
    "                        formatter = logging.Formatter('%(asctime)s - %(message)s')\n",
    "                        new_handler.setFormatter(formatter)\n",
    "                        root_logger.addHandler(new_handler)\n",
    "                        print(\"Image class \", label)\n",
    "                        logging.info(label)\n",
    "                        \n",
    "                        files_label_specific = []\n",
    "                        for j in range(len(file_class_list)):\n",
    "                            if file_class_list[j] == label:\n",
    "                                files_label_specific.append(files[j])\n",
    "                            else:\n",
    "                                pass\n",
    "                        \n",
    "                        tm = int(round(time.time()))\n",
    "                        #name = 'IC_'+label+'_OI_'+str(tm) Removed for better naming convention \n",
    "                        cl_mdl_name = name+'_Class_'+label+'_OI'\n",
    "                        print(\"Class specific OI model \", cl_mdl_name)\n",
    "                        current_version = 1\n",
    "                        parent_id = -1\n",
    "                        ail_model_id = -1\n",
    "                        ail_version_id = -1\n",
    "                        mode_type = \"NA\"   \n",
    "                \n",
    "                        new_model_id = add_model(created_by, files_label_specific, parent_id, cl_mdl_name, solution_id, 'Object Identification', int(current_version), language, mode_type, description, epochs, ail_model_id, ail_version_id, flavour=MLModelFlavours.keras)\n",
    "                        \n",
    "                        img_rescale_flag = get_OI_rescale_flag(solution_id, files_label_specific, new_model_id, created_by)\n",
    "                        print(\"Image rescale flag \", img_rescale_flag)\n",
    "                        for i in multiple_crop_zone:\n",
    "                            if i['class_name']==label:\n",
    "                                OI_MultiCropFlag = i['multi_crop_flag']\n",
    "                        # to get class wise zone config\n",
    "                        zone_config,status = annotation.get_zone_config(new_model_id,files_label_specific,parent_id,solution_id, created_by)\n",
    "                        print(\" zone_config for OI model \", zone_config)\n",
    "                        update_mosaicml_metadata_info(new_model_id,data={'img_rescale_flag':img_rescale_flag,'OI_MultiCropFlag':OI_MultiCropFlag,'zone_config':zone_config})\n",
    "                        OI_details = {\"model_id\":new_model_id,\"name\":cl_mdl_name,\"files\":files_label_specific, \"model_type\":'Object Identification',\"epochs\":epochs,\"img_rescale_flag\":img_rescale_flag,'OI_MultiCropFlag':OI_MultiCropFlag,'zone_config':zone_config}\n",
    "                        Ensemble_ModelDetails.append(OI_details.copy())\n",
    "\n",
    "                        ICOI_proccess = threading.Thread(target=ModelManager().train_zoning, args=[new_model_id, files_label_specific, parent_id,solution_id,epochs, created_by])\n",
    "                        ICOI_PROGRESS = threading.Thread(target=OI_progress, args =[file_nm_path,new_model_id,epochs])\n",
    "                        ICOI_proccess.start()\n",
    "                        ICOI_PROGRESS.start()\n",
    "                        ICOI_proccess.join()\n",
    "                        ICOI_PROGRESS.join()\n",
    "                        print(\"Training OI follwed by IC Model\")\n",
    "                        model_path = os.path.join(CONFIG['MODEL_BASE_LOCATION'], new_model_id)\n",
    "                        model_file_path = model_path+'/frozen_inference_graph.pb'\n",
    "                        print(\"Object Identiifcation model file path \", model_file_path)\n",
    "      \n",
    "                elif result_status == app_constants.FAILED:\n",
    "                    print(\"Failed to get the AI Tutor data for Object identification. \")\n",
    "                    #print(\"Clearing training event\")\n",
    "                    #if clear_training_event():\n",
    "                        #print(\"Cleared training event\")\n",
    "                    update_mosaicml_metadata_info(new_model_id,data={'status':_app_constants.STATUS_FAILED})\n",
    "                    return app_constants.FAILED                \n",
    "\n",
    "        if thread_start_flag:\n",
    "            print(\"Model Training Started\")\n",
    "            model_id = str(new_model_id)\n",
    "\n",
    "            if train_process and eval(model_type)[0] == \"Object Identification\" and len(eval(model_type)) == 1:\n",
    "                file_nm_path = '/home/mosaic-ai/NER_Training_Service.log'\n",
    "                OI_PROGRESS = threading.Thread(target=OI_progress,args =[file_nm_path, new_model_id, epochs])\n",
    "                train_process.start()\n",
    "                OI_PROGRESS.start()\n",
    "                train_process.join()\n",
    "                OI_PROGRESS.join()\n",
    "            else:\n",
    "                train_process.start()\n",
    "                train_process.join()            \n",
    "                \n",
    "        result_model = {\"status\": \"success\",\n",
    "                        \"data\": app_constants.STATUS_INPROGRESS,\n",
    "                        \"model_id\": str(new_model_id),\n",
    "                        \"CCFlag\": CCFlag,\n",
    "                        \"language\":language,\n",
    "                        \"zone_config\":zone_config,\n",
    "                        \"Ensemble_ModelDetails\":Ensemble_ModelDetails}\n",
    "        \n",
    "        return result_model\n",
    "\n",
    "    except Exception as exception:\n",
    "        if os.path.exists(CONFIG[\"TRAIN_EVENT_FILE\"]):\n",
    "            os.remove(CONFIG[\"TRAIN_EVENT_FILE\"])\n",
    "        #LOGGER.exception(str(exception))\n",
    "        print(str(exception))\n",
    "        return {\"status\": \"failure\", \"error\": str(exception)}\n",
    "\n",
    "TrainModel = train()\n",
    "\n",
    "print(\"Out from Train Model Function\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'failure', 'error': \"'language'\"}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TrainModel\n",
    "Ensemble_ModelDetails = TrainModel['Ensemble_ModelDetails']\n",
    "CCFlag = TrainModel['CCFlag']\n",
    "PREDICTION_FORMAT_DETAILED = 1\n",
    "zone_config = TrainModel['zone_config']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Param\n",
    "'''\n",
    "This code is commented for now, will be deleted as part of cleanup activity\n",
    "\n",
    "PREDICTION_FORMAT_DETAILED = 1\n",
    "\n",
    "model_type = eval(os.getenv('model_type','NOT_SPECIFIED'))\n",
    "print(\"model_type\",model_type)\n",
    "\n",
    "if len(Ensemble_ModelDetails)==0:\n",
    "    model_path = os.path.join(CONFIG['MODEL_BASE_LOCATION'], TrainModel['model_id'])\n",
    "    if 'NER' in model_type:\n",
    "        model_file_path = model_path+'/valid_char_set.pkl'\n",
    "        print(\"model_file_path...............\",model_file_path)\n",
    "\n",
    "    if 'DC' in model_type:\n",
    "        file_path = '%s_document_classification_svc_model.pkl' % TrainModel['model_id']\n",
    "        model_file_path = model_path+'/'+file_path\n",
    "        print(\"model_file_path...............\",model_file_path)\n",
    "\n",
    "    if 'IC' in model_type:\n",
    "        model_file_path = model_path+'/model_complete.h5'\n",
    "        print(\"model_file_path...............\",model_file_path)\n",
    "\n",
    "    if 'Object Identification' in model_type :\n",
    "        model_file_path = model_path+'/frozen_inference_graph.pb'\n",
    "        print(\"model_file_path...............\",model_file_path)\n",
    "        # to get OI image rescale flag\n",
    "        img_rescale_flag = get_OI_rescale_flag(solution_id,files,new_model_id, created_by)\n",
    "        print(\" img_rescale_flag and updating metadata\",img_rescale_flag)\n",
    "        update_mosaicml_metadata_info(new_model_id,data={'img_rescale_flag':img_rescale_flag})\n",
    "        \n",
    "    print(\"PREDICTION_FORMAT_DETAILED......\",PREDICTION_FORMAT_DETAILED)\n",
    "    print(\"model_id......\",TrainModel['model_id'])\n",
    "    print(\"model_path......\",model_path)\n",
    "\n",
    "    if os.path.isfile(model_file_path):\n",
    "        print(\"recived new files\")\n",
    "        time.sleep(30)\n",
    "    else:\n",
    "        raise ValueError(\"%s isn't a file!\" % file_path)\n",
    "'''        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Inference Flow "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sents_and_words(text,language='english'):\n",
    "    \"\"\"\n",
    "    Split the text into sentences containing list of tokens.\n",
    "\n",
    "    Arguments :\n",
    "    text    - input text as a string.\n",
    "\n",
    "    Returns :\n",
    "    List of sentences where each sentence is further a list of tokens.\n",
    "    \"\"\"\n",
    "    content = text\n",
    "    content = re.sub(r\"(?<= [.(a-zA-z]{3})\\.(?!=(\\n))\", ' ', content)\n",
    "    content = re.sub(r\"(?<= [a-zA-z]{2})\\.(?!=(\\n))\", ' ', content)\n",
    "\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "    \n",
    "    if language in nltk_supported_lang:        \n",
    "        for sent in sent_tokenize(content,language=language):\n",
    "            for word in word_tokenize(sent,language=language):\n",
    "                # Fix nltk word tokenize issue for double quotes.\n",
    "                word = word.replace(\"''\", '\"').replace('``', '\"')\n",
    "                sentence.append([word, word])\n",
    "\n",
    "            if len(sentence) > 0:\n",
    "                sentences.append(sentence)\n",
    "                sentence = []\n",
    "    if language=='mongolian':\n",
    "        from polyglot.text import Text\n",
    "        zen = Text(text)\n",
    "        sentences_poly = zen.sentences\n",
    "        sentences_poly = [elem.string for elem in sentences_poly]\n",
    "        # sentences_poly\n",
    "        for sent in sentences_poly:\n",
    "            for word in Text(sent).words:\n",
    "                word = word.replace(\"''\", '\"').replace('``', '\"')\n",
    "                sentence.append([word, word])\n",
    "            if len(sentence) > 0:\n",
    "                sentences.append(sentence)\n",
    "                sentence = []   \n",
    "    return sentences\n",
    "\n",
    "def add_char_informatioin_evaluate(sentences_in):\n",
    "    \"\"\"Add character level features to words of sentences.\"\"\"\n",
    "    sentences = sentences_in.copy()\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        for j, data in enumerate(sentence):\n",
    "            chars = [c for c in data[0]]\n",
    "            sentences[i][j] = [data[0], chars]\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def padding(sentences_in, maxlen=52):\n",
    "    \"\"\"Pad sequences upto max length specified or the max length in data.\"\"\"\n",
    "    sentences = sentences_in.copy()\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        sentences[i][2] = pad_sequences(sentences[i][2],\n",
    "                                        maxlen,\n",
    "                                        padding='post')\n",
    "    return sentences\n",
    "\n",
    "def create_data_matrices(sentences, word2idx, case2idx, char2idx):\n",
    "    \"\"\"\n",
    "    Create numerical arrays for textual data.\n",
    "\n",
    "    Arguments :\n",
    "    sentences   - list of sentences containing word tokens along with\n",
    "                  word and characterfs level features.\n",
    "    word2idx    - dictionary containing word to id mapping.\n",
    "    case2idx    - dictionary containing case to id mapping.\n",
    "    char2idx    - dictionary containing character to id mapping.\n",
    "\n",
    "    Returns :\n",
    "    Dataset with text replaced with numerical ids.\n",
    "    \"\"\"\n",
    "    unknown_idx = word2idx['UNKNOWN_TOKEN']\n",
    "\n",
    "    dataset = []\n",
    "\n",
    "    word_count = 0\n",
    "    unknown_word_count = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        word_indices = []\n",
    "        case_indices = []\n",
    "        char_indices = []\n",
    "\n",
    "        for word, char in sentence:\n",
    "            word_count += 1\n",
    "            if word in word2idx:\n",
    "                word_idx = word2idx[word]\n",
    "            elif word.lower() in word2idx:\n",
    "                word_idx = word2idx[word.lower()]\n",
    "            else:\n",
    "                word_idx = unknown_idx\n",
    "                unknown_word_count += 1\n",
    "            char_idx = []\n",
    "            for x in char:\n",
    "                char_idx.append(char2idx[x])\n",
    "            # Get the label and map to int\n",
    "            word_indices.append(word_idx)\n",
    "            case_indices.append(get_casing(word, case2idx))\n",
    "            char_indices.append(char_idx)\n",
    "\n",
    "        dataset.append([word_indices, case_indices, char_indices])\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def get_casing(word, case_lookup):\n",
    "    \"\"\"Return the case id for given word from case lookup provided.\"\"\"\n",
    "    casing = 'other'\n",
    "    if len(word) == 0:\n",
    "        return case_lookup[casing]\n",
    "    num_digits = 0\n",
    "    for char in word:\n",
    "        if char.isdigit():\n",
    "            num_digits += 1\n",
    "\n",
    "    digit_fraction = num_digits / float(len(word))\n",
    "\n",
    "    if word.isdigit():  # Is a digit\n",
    "        casing = 'numeric'\n",
    "    elif digit_fraction > 0.5:\n",
    "        casing = 'mainly_numeric'\n",
    "    elif word.islower():  # All lower case\n",
    "        casing = 'allLower'\n",
    "    elif word.isupper():  # All upper case\n",
    "        casing = 'allUpper'\n",
    "    elif word[0].isupper():  # is a title, initial char upper, then all lower\n",
    "        casing = 'initialUpper'\n",
    "    elif num_digits > 0:\n",
    "        casing = 'contains_digit'\n",
    "    return case_lookup[casing]\n",
    "\n",
    "def tag_dataset_evaluate(dataset, model):\n",
    "    \"\"\"\n",
    "    Get the inference of the given model on the given dataset.\n",
    "\n",
    "    Arguments :\n",
    "    dataset  - list/tuple containing data the format (tokens, casing, char).\n",
    "    model    - model to be used for prediction.\n",
    "\n",
    "    Returns :\n",
    "        Tuple of predicted labels and respective scores\n",
    "    \"\"\"\n",
    "    pred_labels = []\n",
    "    b = Progbar(len(dataset))\n",
    "    score = []\n",
    "    for i, data in enumerate(dataset):\n",
    "        tokens, casing, char = data\n",
    "        tokens = np.asarray([tokens])\n",
    "        casing = np.asarray([casing])\n",
    "        char = np.asarray([char])\n",
    "        pred = model.predict([tokens, casing, char], verbose=False)[0]\n",
    "        score = score + list(pred.max(axis=-1))\n",
    "        pred_index = pred.argmax(axis=-1)\n",
    "        pred_labels.append(pred_index)\n",
    "        b.update(i)\n",
    "    return pred_labels, score\n",
    "\n",
    "def get_inference_run(input_data_dict, prediction_format=0, model={},language=\"english\", k_graph=None, entity_order_map=None, root_level_ents=None, use_roi=False):\n",
    "    \"\"\"\n",
    "    Run the inference and return the predictions.\n",
    "\n",
    "    Keyword arguments:\n",
    "    model    - dictionary with keras model available as key \"model\"\n",
    "    text     - input to the model\n",
    "    prediction_format - if set to 1, returns detailed format.\n",
    "                        By default it is set to 0.\n",
    "    use_roi  - boolean specifying the use of content classification\n",
    "    \"\"\"\n",
    "    \n",
    "    annotations_result = []\n",
    "    # Added for Content Classification\n",
    "    \n",
    "    for page_no in sorted(input_data_dict.keys(),\n",
    "                          key=lambda x: int(x)):\n",
    "        for sent_no in sorted(input_data_dict[page_no].keys(),\n",
    "                              key=lambda x: int(x)):\n",
    "            temp_array = []\n",
    "            text = input_data_dict[page_no][sent_no][\"sentence\"]\n",
    "            unique_id = input_data_dict[page_no][sent_no][\"unique_id\"]\n",
    "            if language in latin_encoding_lang :\n",
    "                print(\" language \",language)\n",
    "                text = remove_non_ascii_2(text)\n",
    "            if language in utf_encoding_lang:\n",
    "                print(\" language \",language)\n",
    "                text = text\n",
    "            copied_variable_text = text\n",
    "            text = tokenize_sents_and_words(text,language)\n",
    "            text = add_char_informatioin_evaluate(text)\n",
    "            test_text = text\n",
    "            text = padding(create_data_matrices(text,word2idx, case2idx,char2idx))\n",
    "            test_batch, test_batch_len = create_batches(text)\n",
    "            predLabels, sentences_score = tag_dataset_evaluate(test_batch,model)\n",
    "            entities = []\n",
    "            for i in predLabels:\n",
    "                for v in i:\n",
    "                    entities.append(list(label2idx.keys())[\n",
    "                        list(label2idx.values()).index(v)])\n",
    "            keyindex = 0\n",
    "            for sent in test_text:\n",
    "                word_index = 0\n",
    "                for word in sent:\n",
    "                    temp = {}\n",
    "                    temp[word[0]] = re.sub('\\n', '', entities[keyindex])\n",
    "                    temp_array.append((temp, sentences_score[word_index]))\n",
    "                    keyindex = keyindex + 1\n",
    "                    word_index = word_index + 1\n",
    "\n",
    "            annotations = []\n",
    "            if prediction_format == PREDICTION_FORMAT_DETAILED:\n",
    "                annotations = prediction_index_format(temp_array,\n",
    "                                                      copied_variable_text,\n",
    "                                                      page_no,\n",
    "                                                      sent_no,\n",
    "                                                      unique_id, language=language)\n",
    "            else:\n",
    "                annotations = temp_array\n",
    "\n",
    "            if len(annotations) > 0:\n",
    "                annotations_result.extend(annotations)\n",
    "\n",
    "    print(\"annotations_result\")\n",
    "#     print(\"k_graph \",k_graph,type(k_graph))\n",
    "#     print(\"entity_order_map \",entity_order_map,type(entity_order_map))\n",
    "#     print(\"root_level_ents \",root_level_ents,type(root_level_ents))\n",
    "    annotations_result = extract_entity_relation(\n",
    "        annotations_result, k_graph, entity_order_map, root_level_ents)\n",
    "    return annotations_result\n",
    "\n",
    "def get_region_of_interest_AIL(tf_idf_model,svc_clf, file_data):\n",
    "    \n",
    "    file_data_df = pd.DataFrame(file_data, columns=['page_number', 'page_data'])\n",
    "    transformed_vector_text = tf_idf_model.transform(file_data_df['page_data'])\n",
    "    final_pred = svc_clf.predict(transformed_vector_text)\n",
    "    pred_probability = svc_clf.predict_proba(transformed_vector_text)\n",
    "\n",
    "    file_data_df['classification_flag'] = final_pred\n",
    "    file_data_df['zero_probability'] = pred_probability[:, 0]\n",
    "    file_data_df['final_classification_flag'] = np.where(\n",
    "        ((file_data_df.zero_probability < CONFIG['CC_ZERO_PROB_THRESHOLD'])\n",
    "         & (file_data_df.classification_flag == 0)),\n",
    "        1,\n",
    "        file_data_df.classification_flag)\n",
    "    classified_pages = file_data_df[\n",
    "        file_data_df['final_classification_flag'] == 1].page_number.tolist()\n",
    "\n",
    "    return classified_pages\n",
    "\n",
    "def get_inference_run_CC(input_data_dict,tf_idf_model,svc_clf,prediction_format=0, model={}, k_graph=None, entity_order_map=None, root_level_ents=None,language=\"english\" ,use_roi=False):\n",
    "    \"\"\"\n",
    "    Run the inference and return the predictions.\n",
    "\n",
    "    Keyword arguments:\n",
    "    model    - dictionary with keras model available as key \"model\"\n",
    "    text     - input to the model\n",
    "    prediction_format - if set to 1, returns detailed format.\n",
    "                        By default it is set to 0.\n",
    "    use_roi  - boolean specifying the use of content classification\n",
    "    \"\"\"\n",
    "    \n",
    "    annotations_result = []\n",
    "    # Added for Content Classification\n",
    "    roi_input = []\n",
    "    for page_number, value in input_data_dict.items():\n",
    "        page_text = \"\"\n",
    "        for sentence_num in sorted(value.keys(),\n",
    "                                   key=lambda x: int(x)):\n",
    "            page_text = page_text + \" \" + value[sentence_num]['sentence']\n",
    "            # page_text_without_asc = remove_non_ascii_2(value[sentence_num]['sentence'])\n",
    "            # page_text = page_text + \" \" + page_text_without_asc\n",
    "        roi_input.append([page_number, page_text])\n",
    "    roi_page_numbers = get_region_of_interest_AIL(tf_idf_model,svc_clf, roi_input)\n",
    "    print('valid page numbers', roi_page_numbers)\n",
    "    \n",
    "    for page_no in sorted(input_data_dict.keys(),\n",
    "                          key=lambda x: int(x)):\n",
    "        if page_no not in roi_page_numbers:\n",
    "            continue\n",
    "        for sent_no in sorted(input_data_dict[page_no].keys(),\n",
    "                              key=lambda x: int(x)):\n",
    "            temp_array = []\n",
    "            text = input_data_dict[page_no][sent_no][\"sentence\"]\n",
    "            unique_id = input_data_dict[page_no][sent_no][\"unique_id\"]\n",
    "            if language in latin_encoding_lang:\n",
    "                print(\" language \",language)\n",
    "                text = remove_non_ascii_2(text)\n",
    "            if language in utf_encoding_lang:\n",
    "                print(\" language \",language)\n",
    "                text = text\n",
    "            copied_variable_text = text\n",
    "            text = tokenize_sents_and_words(text,language)\n",
    "            text = add_char_informatioin_evaluate(text)\n",
    "            test_text = text\n",
    "            text = padding(create_data_matrices(text,word2idx, case2idx,char2idx))\n",
    "            test_batch, test_batch_len = create_batches(text)\n",
    "            predLabels, sentences_score = tag_dataset_evaluate(test_batch,model)\n",
    "            entities = []\n",
    "            for i in predLabels:\n",
    "                for v in i:\n",
    "                    entities.append(list(label2idx.keys())[\n",
    "                        list(label2idx.values()).index(v)])\n",
    "            keyindex = 0\n",
    "            for sent in test_text:\n",
    "                word_index = 0\n",
    "                for word in sent:\n",
    "                    temp = {}\n",
    "                    temp[word[0]] = re.sub('\\n', '', entities[keyindex])\n",
    "                    temp_array.append((temp, sentences_score[word_index]))\n",
    "                    keyindex = keyindex + 1\n",
    "                    word_index = word_index + 1\n",
    "\n",
    "            annotations = []\n",
    "            if prediction_format == PREDICTION_FORMAT_DETAILED:\n",
    "                annotations = prediction_index_format(temp_array,\n",
    "                                                      copied_variable_text,\n",
    "                                                      page_no,\n",
    "                                                      sent_no,\n",
    "                                                      unique_id, language=language)\n",
    "            else:\n",
    "                annotations = temp_array\n",
    "\n",
    "            if len(annotations) > 0:\n",
    "                annotations_result.extend(annotations)\n",
    "    annotations_result = extract_entity_relation(\n",
    "        annotations_result, k_graph, entity_order_map, root_level_ents)\n",
    "    return annotations_result\n",
    "\n",
    "def remove_non_ascii_2(text):\n",
    "    \"\"\"Return ASCII version of the text.\"\"\"\n",
    "    return re.sub(r'[^\\x1F-\\x7F]|[`]', ' ', text)\n",
    "\n",
    "\n",
    "def create_batches(data):\n",
    "    \"\"\"\n",
    "    Create batches by groping together same length records.\n",
    "\n",
    "    Arguments :\n",
    "    data   - List of data in the format format (tokens, casing, char)\n",
    "\n",
    "    Returns :\n",
    "    data along with list containing running batch lengths of different batches.\n",
    "    \"\"\"\n",
    "    unique_len_of_data = []\n",
    "    for i in data:\n",
    "        unique_len_of_data.append(len(i[0]))\n",
    "    unique_len_of_data = set(unique_len_of_data)\n",
    "    batches = []\n",
    "    batch_len = []\n",
    "    z = 0\n",
    "    for i in unique_len_of_data:\n",
    "        for batch in data:\n",
    "            if len(batch[0]) == i:\n",
    "                batches.append(batch)\n",
    "                z += 1\n",
    "        batch_len.append(z)\n",
    "    return batches, batch_len\n",
    "\n",
    "# New code to fix index out of list issue - Pranav\n",
    "\n",
    "def prediction_index_format(standard_prediction, plain_text, page_no, sent_no, unique_id, language=language):\n",
    "    \"\"\"\n",
    "    Add start and end index to every word in model prediction.\n",
    "\n",
    " \n",
    "\n",
    "    Arguments:\n",
    "        standard_prediction -  standard output of the model\n",
    "        plain text:  input text given to the model\n",
    "    Returns :\n",
    "        Json of words with starting,ending index and confidence score\n",
    "    \"\"\"\n",
    "    # merge IOB data\n",
    "    print(\"standard Prediction ----------\")\n",
    "    iob_data = []\n",
    "    for prediction_with_score in standard_prediction:\n",
    "        prediction = prediction_with_score[0]\n",
    "        word, annotation = list(prediction.items())[0]\n",
    "        iob_data.append((word, annotation, annotation))\n",
    "\n",
    " \n",
    "\n",
    "    ne_tree = conlltags2tree(iob_data)\n",
    "    print(\"-------------ne_tree--------\")\n",
    "    annotations = []\n",
    "    plain_text_length = len(plain_text)\n",
    "    starting_index = 0\n",
    "    character_count = 0\n",
    "    found_length = 0\n",
    "    annotations = []\n",
    "\n",
    " \n",
    "\n",
    "    standard_prediction_index = 0\n",
    "    for x in ne_tree:\n",
    "        if isinstance(x, nltk.tree.Tree):\n",
    "            temp_dict = {}\n",
    "            temp_dict['annotation'] = x.label()\n",
    "            temp_dict['page_no'] = page_no\n",
    "            temp_dict['sentence_no'] = sent_no\n",
    "            temp_dict['unique_id'] = unique_id\n",
    "            temp_word_str = ''\n",
    "            temp_starting_index = []\n",
    "            temp_end_position = []\n",
    "            temp_score = []\n",
    "            temp_word_list = []\n",
    "            for token, pos in x.leaves():\n",
    "                standard_prediction_index += 1\n",
    "                temp_word = ''\n",
    "                space = ''\n",
    "\n",
    "                for index in range(starting_index, plain_text_length):\n",
    "                    temp_word = temp_word + plain_text[index]\n",
    "                    if plain_text[index] == \" \":\n",
    "                        temp_word = ''\n",
    "                        space = ' '\n",
    "                        found_length = index - 1\n",
    "                        starting_index = index + 1  \n",
    "                    # added below fix for utf_encoding_lang    \n",
    "                    if temp_word.isspace()and language in utf_encoding_lang :\n",
    "                        temp_word = ''\n",
    "                        space = ' '\n",
    "                        found_length = index - 1                      \n",
    "                        starting_index = index + 1\n",
    "                    if (temp_word == token):\n",
    "                        word_length = len(token)\n",
    "                        found_length = index - word_length\n",
    "\n",
    "                        temp_word_str = temp_word_str + space + token\n",
    "                        space = ''\n",
    "\n",
    "                        temp_word_list.append(token)\n",
    "                        temp_starting_index.append(found_length + 1)\n",
    "                        temp_end_position.append(found_length + word_length)\n",
    "                        try: \n",
    "                            temp_score.append(str(standard_prediction[standard_prediction_index][1]))\n",
    "                        except:\n",
    "                            temp_score.append(\"0\")\n",
    "\n",
    "                        temp_word = ''\n",
    "                        starting_index = index + 1\n",
    "                        break\n",
    "                    character_count = character_count + 1\n",
    "            temp_dict['word'] = temp_word_str.strip()\n",
    "            temp_dict['word_list'] = temp_word_list\n",
    "            temp_dict['starting_index'] = temp_starting_index\n",
    "            temp_dict['end_position'] = temp_end_position\n",
    "            temp_dict['score'] = temp_score\n",
    "            temp_dict['word_index'] = []\n",
    "            #---- Added condition to remove blank entities extracted by model ----#\n",
    "            if temp_dict['word'] and temp_dict['word_list'] and temp_dict['score']:\n",
    "                annotations.append(temp_dict)\n",
    "            #annotations.append(temp_dict)\n",
    "\n",
    "    # word index for entities\n",
    "    sent_word_token = custom_word_tokinezer(plain_text, language=language)\n",
    "\n",
    "    pad_word_index = 0\n",
    "    print(\"Pred_ind_format annotations\")\n",
    "    for annotation in annotations:\n",
    "        tmp_word_list = annotation['word_list']\n",
    "\n",
    "        for word_token in tmp_word_list:\n",
    "            temp_word_index = find_element_in_list(word_token, sent_word_token)\n",
    "\n",
    "            if temp_word_index is not None:\n",
    "                sent_word_token = sent_word_token[temp_word_index:]\n",
    "                pad_word_index += temp_word_index\n",
    "                annotation['word_index'].append(pad_word_index)\n",
    "\n",
    "    return annotations\n",
    "\n",
    "\n",
    "def custom_word_tokinezer(plain_text, language=language):\n",
    "    \"\"\"Tokenize text while preserving the white spaces.\"\"\"\n",
    "    if language==\"mongolian\":\n",
    "        from polyglot.text import Text\n",
    "        zen = Text(plain_text)\n",
    "        tokens = zen.words\n",
    "    else:\n",
    "        tokens = word_tokenize(plain_text, language=language)\n",
    "\n",
    "    # Handle nltk tokenizer issue with double quotes.\n",
    "    tokens = [x.replace(\"''\", '\"').replace('``', '\"') for x in tokens]\n",
    "    plain_text_length = len(plain_text)\n",
    "    starting_index = 0\n",
    "\n",
    "    word_token_list = []\n",
    "\n",
    "    for token in tokens:\n",
    "        temp_word = ''\n",
    "        for index in range(starting_index, plain_text_length):\n",
    "            temp_word = temp_word + plain_text[index]\n",
    "            if re.match(r\"[\\s]+\", plain_text[index]):\n",
    "                word_token_list.append(plain_text[index])\n",
    "                temp_word = ''\n",
    "            if (temp_word == token):\n",
    "                word_token_list.append(temp_word)\n",
    "                temp_word = ''\n",
    "                starting_index = index + 1\n",
    "                break\n",
    "\n",
    "    return word_token_list\n",
    "\n",
    "def find_element_in_list(element, list_element):\n",
    "    \"\"\"Find element in list of elements.\"\"\"\n",
    "    try:\n",
    "        index_element = list_element.index(element)\n",
    "        return index_element\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "# def extract_entity_relation(ner_model_output, knowledge_base,\n",
    "#                             entity_order_map, root_level_ents=None):\n",
    "#     \"\"\"Format the output of model as a knowledge graph.\"\"\"\n",
    "#     di_rel_graph = nx.from_edgelist(knowledge_base, create_using=nx.DiGraph())\n",
    "#     valid_entities = set(entity_order_map.keys())\n",
    "#     entity_level_map = get_group_mapping(di_rel_graph)\n",
    "#     model_output_queue = ner_model_output[::-1]\n",
    "#     '''\n",
    "#     Commented out this for bug and fix added in loop below\n",
    "#     if len(ner_model_output)>0 and root_level_ents==None:\n",
    "#         found_valid_entity = False\n",
    "#         while not found_valid_entity:\n",
    "#             first_element = model_output_queue.pop()\n",
    "#             if first_element['annotation'] in valid_entities:\n",
    "#                 found_valid_entity = True\n",
    "#     '''\n",
    "#     if len(ner_model_output)>0 and root_level_ents != None:\n",
    "#         found_valid_entity = False\n",
    "#         while not found_valid_entity:\n",
    "#             for element in model_output_queue:\n",
    "#                 if element['annotation'] in root_level_ents:\n",
    "#                     first_element = element\n",
    "#                     model_output_queue.remove(element)\n",
    "#                     found_valid_entity = True\n",
    "#                     break\n",
    "    \n",
    "#         global_ents_inverse = set(y for x in knowledge_base for y in x)\n",
    "#         output = OrderedDict()\n",
    "#         entity_type_vs_key_mapping = []\n",
    "#         output[first_element['word']] = {'attribute_name': first_element[\n",
    "#             'annotation']}\n",
    "#         output[first_element['word']]['annotation'] = first_element\n",
    "#         output[first_element['word']]['attribute_type'] = entity_level_map.get(\n",
    "#             first_element['annotation'], 'global')\n",
    "#         output[first_element['word']]['value'] = first_element['word']\n",
    "#         output[first_element['word']]['order'] = entity_order_map.get(\n",
    "#             first_element['annotation'])\n",
    "#         entity_type_vs_key_mapping.append(\n",
    "#             (first_element['annotation'], (first_element['word'],)))\n",
    "\n",
    "#         dummy_var_count = 1\n",
    "#         safety_counter = 0\n",
    "#         threshold = 10000\n",
    "\n",
    "#         while len(model_output_queue) > 0:\n",
    "#             if safety_counter >= threshold:\n",
    "#                 break\n",
    "#             safety_counter += 1\n",
    "#             entity = model_output_queue.pop()\n",
    "#             curr_entity_value = entity['word']\n",
    "#             curr_entity_type = entity['annotation']\n",
    "\n",
    "#             if curr_entity_type not in valid_entities:\n",
    "#                 continue\n",
    "\n",
    "#             curr_key = curr_entity_value\n",
    "#             if curr_entity_type not in global_ents_inverse:\n",
    "#                 output[curr_key] = {'attribute_name': curr_entity_type}\n",
    "#                 output[curr_key]['annotation'] = entity\n",
    "#                 output[curr_key]['attribute_type'] = \"global\"\n",
    "#                 output[curr_key]['value'] = curr_entity_value\n",
    "#                 output[curr_key]['order'] = entity_order_map.get(curr_entity_type)\n",
    "#                 continue\n",
    "\n",
    "#             mapping_loop_over_copy = copy.deepcopy(entity_type_vs_key_mapping)\n",
    "#             for i in range(len(entity_type_vs_key_mapping)):\n",
    "#                 prev_entity_type, prev_entity_keys = entity_type_vs_key_mapping[-1]\n",
    "#                 #added below line to resolve single quote issue\n",
    "#                 prev_entity_keys = tuple([prev_entity_keys[0].replace(\"'\", \"\\\\\\'\")])\n",
    "#                 print(\"prev_entity_keys  \",prev_entity_keys)\n",
    "                \n",
    "#                 match_found = False\n",
    "#                 if di_rel_graph.has_successor(prev_entity_type, curr_entity_type):\n",
    "#                     match_found = True\n",
    "#                     prev_key = \"['\" + \"']['\".join(prev_entity_keys) + \"']\"\n",
    "#                     qry_e_type = '{}{}[\"\"\"{}\"\"\"]={}'.format(\n",
    "#                         'output', prev_key, curr_key,\n",
    "#                         {'attribute_name': curr_entity_type})\n",
    "#                     qry_annotation = '{}{}[\"\"\"{}\"\"\"][\"annotation\"]={}'.format(\n",
    "#                         'output', prev_key, curr_key, 'entity')\n",
    "#                     qry_attribute_type = '{}{}[\"\"\"{}\"\"\"][\"attribute_type\"]=\"{}\"'.format(\n",
    "#                         'output', prev_key, curr_key,\n",
    "#                         entity_level_map[curr_entity_type])\n",
    "#                     qry_value = '{}{}[\"\"\"{}\"\"\"][\"value\"]=\"\"\"{}\"\"\"'.format(\n",
    "#                         'output', prev_key, curr_key,\n",
    "#                         \"\" if (int(entity['page_no']) == 0) else curr_entity_value)\n",
    "#                     qry_order = '{}{}[\"\"\"{}\"\"\"][\"order\"]=\"{}\"'.format(\n",
    "#                         'output', prev_key, curr_key,\n",
    "#                         entity_order_map.get(curr_entity_type))\n",
    "#                     exec(qry_e_type)\n",
    "#                     exec(qry_annotation)\n",
    "#                     exec(qry_attribute_type)\n",
    "#                     exec(qry_value)\n",
    "#                     exec(qry_order)\n",
    "#                     entity_type_vs_key_mapping.append(\n",
    "#                         (curr_entity_type, prev_entity_keys + (curr_key,)))\n",
    "#                     break\n",
    "#                 entity_type_vs_key_mapping.pop()\n",
    "#             if not match_found:\n",
    "#                 if len(tuple(di_rel_graph.predecessors(curr_entity_type))) == 0:\n",
    "#                     entity_type_vs_key_mapping.append(\n",
    "#                         (curr_entity_type, (curr_key,)))\n",
    "#                     output[curr_key] = {'attribute_name': curr_entity_type}\n",
    "#                     output[curr_key]['annotation'] = entity\n",
    "#                     output[curr_key]['attribute_type'] = entity_level_map.get(\n",
    "#                         output[curr_key]['attribute_name'])\n",
    "#                     output[curr_key]['value'] = (\"\" if int(entity[\n",
    "#                         'page_no']) == 0 else curr_entity_value)\n",
    "#                     output[curr_key]['order'] = entity_order_map.get(\n",
    "#                         curr_entity_type)\n",
    "#                 else:\n",
    "#                     dummy_key = 'dummy_' + str(dummy_var_count)\n",
    "#                     dummy_entity = create_dummy_entity(\n",
    "#                         dummy_entity_name=dummy_key,\n",
    "#                         curr_entity_type=curr_entity_type,\n",
    "#                         di_rel_graph=di_rel_graph)\n",
    "#                     dummy_var_count += 1\n",
    "#                     model_output_queue.append(entity)\n",
    "#                     model_output_queue.append(dummy_entity)\n",
    "#                     entity_type_vs_key_mapping = mapping_loop_over_copy\n",
    "\n",
    "#         output_formatted = []\n",
    "#         for key, value in output.items():\n",
    "#             output_formatted.append(value)\n",
    "#         for d in output_formatted:\n",
    "#             recursive_dict_iterator(d)\n",
    "#         # Fill in the blank entities in the output and sort as per sort order\n",
    "#         fill_blank_entities_in_k_graph(\n",
    "#             output_formatted, di_rel_graph, entity_level_map,\n",
    "#             entity_order_map, root_level_ents)\n",
    "        \n",
    "#     else:\n",
    "#         output_formatted = []\n",
    "#         fill_blank_entities_in_k_graph(\n",
    "#             output_formatted, di_rel_graph, entity_level_map,\n",
    "#             entity_order_map, root_level_ents)\n",
    "\n",
    "#     return output_formatted\n",
    "\n",
    "def extract_entity_relation(ner_model_output, knowledge_base,\n",
    "                            entity_order_map, root_level_ents=None):\n",
    "    \"\"\"Format the output of model as a knowledge graph.\"\"\"\n",
    "    di_rel_graph = nx.from_edgelist(knowledge_base, create_using=nx.DiGraph())\n",
    "    valid_entities = set(entity_order_map.keys())\n",
    "    entity_level_map = get_group_mapping(di_rel_graph)\n",
    "    model_output_queue = ner_model_output[::-1]\n",
    "\n",
    "    if len(ner_model_output)>0 and root_level_ents != None:\n",
    "        found_valid_entity = False\n",
    "        while not found_valid_entity:\n",
    "            for element in model_output_queue:\n",
    "                if element['annotation'] in root_level_ents:\n",
    "                    first_element = element\n",
    "                    model_output_queue.remove(element)\n",
    "                    found_valid_entity = True\n",
    "                    break\n",
    "        \n",
    "        dummy_var_count = 1\n",
    "        safety_counter = 0\n",
    "        threshold = 10000\n",
    "        \n",
    "        \n",
    "        global_ents_inverse = set(y for x in knowledge_base for y in x) # all group entities\n",
    "        #print(global_ents_inverse)\n",
    "        output = OrderedDict()\n",
    "        entity_type_vs_key_mapping = []\n",
    "        output[first_element['word']+'___'+first_element['annotation']+'___'+str(safety_counter)] = {'attribute_name': first_element[\n",
    "            'annotation']}\n",
    "        output[first_element['word']+'___'+first_element['annotation']+'___'+str(safety_counter)]['annotation'] = first_element\n",
    "        output[first_element['word']+'___'+first_element['annotation']+'___'+str(safety_counter)]['attribute_type'] = entity_level_map.get(\n",
    "            first_element['annotation'], 'global')\n",
    "        output[first_element['word']+'___'+first_element['annotation']+'___'+str(safety_counter)]['value'] = first_element['word']\n",
    "        output[first_element['word']+'___'+first_element['annotation']+'___'+str(safety_counter)]['order'] = entity_order_map.get(\n",
    "            first_element['annotation'])\n",
    "        \n",
    "#         entity_type_vs_key_mapping.append(\n",
    "#             (first_element['annotation'], (first_element['word'],)))\n",
    "        entity_type_vs_key_mapping.append(\n",
    "            (first_element['annotation'], (first_element['word']+'___'+first_element['annotation']+'___'+str(safety_counter),)))\n",
    "\n",
    "        \n",
    "\n",
    "        while len(model_output_queue) > 0:\n",
    "            if safety_counter >= threshold:\n",
    "                break\n",
    "            safety_counter += 1\n",
    "            entity = model_output_queue.pop()\n",
    "            curr_entity_value = entity['word']\n",
    "            curr_entity_type = entity['annotation']\n",
    "\n",
    "            if curr_entity_type not in valid_entities:\n",
    "                continue\n",
    "\n",
    "            curr_key = curr_entity_value +'___'+curr_entity_type +'___'+str(safety_counter)\n",
    "            if curr_entity_type not in global_ents_inverse:\n",
    "                output[curr_key] = {'attribute_name': curr_entity_type}\n",
    "                output[curr_key]['annotation'] = entity\n",
    "                output[curr_key]['attribute_type'] = \"global\"\n",
    "                output[curr_key]['value'] = curr_entity_value\n",
    "                output[curr_key]['order'] = entity_order_map.get(curr_entity_type)\n",
    "                continue\n",
    "            \n",
    "            mapping_loop_over_copy = copy.deepcopy(entity_type_vs_key_mapping)\n",
    "            for i in range(len(entity_type_vs_key_mapping)):\n",
    "                prev_entity_type, prev_entity_keys = entity_type_vs_key_mapping[-1]\n",
    "                #added below line to resolve single quote issue\n",
    "                prev_entity_keys = tuple([prev_entity_keys[0].replace(\"'\", \"\\\\\\'\")])\n",
    "#                 print(\"prev_entity_keys  \",prev_entity_keys)\n",
    "                \n",
    "                match_found = False\n",
    "                if di_rel_graph.has_successor(prev_entity_type, curr_entity_type):\n",
    "                    match_found = True\n",
    "                    prev_key = \"['\" + \"']['\".join(prev_entity_keys) + \"']\"\n",
    "                    qry_e_type = '{}{}[\"\"\"{}\"\"\"]={}'.format(\n",
    "                        'output', prev_key, curr_key,\n",
    "                        {'attribute_name': curr_entity_type})\n",
    "                    qry_annotation = '{}{}[\"\"\"{}\"\"\"][\"annotation\"]={}'.format(\n",
    "                        'output', prev_key, curr_key, 'entity')\n",
    "                    qry_attribute_type = '{}{}[\"\"\"{}\"\"\"][\"attribute_type\"]=\"{}\"'.format(\n",
    "                        'output', prev_key, curr_key,\n",
    "                        entity_level_map[curr_entity_type])\n",
    "                    qry_value = '{}{}[\"\"\"{}\"\"\"][\"value\"]=\"\"\"{}\"\"\"'.format(\n",
    "                        'output', prev_key, curr_key,\n",
    "                        \"\" if (int(entity['page_no']) == 0) else curr_entity_value)\n",
    "                    qry_order = '{}{}[\"\"\"{}\"\"\"][\"order\"]=\"{}\"'.format(\n",
    "                        'output', prev_key, curr_key,\n",
    "                        entity_order_map.get(curr_entity_type))\n",
    "                    exec(qry_e_type)\n",
    "                    exec(qry_annotation)\n",
    "                    exec(qry_attribute_type)\n",
    "                    exec(qry_value)\n",
    "                    exec(qry_order)\n",
    "                    entity_type_vs_key_mapping.append(\n",
    "                        (curr_entity_type, prev_entity_keys + (curr_key,)))\n",
    "                    break\n",
    "                entity_type_vs_key_mapping.pop()\n",
    "            \n",
    "            if not match_found:\n",
    "                if len(tuple(di_rel_graph.predecessors(curr_entity_type))) == 0:\n",
    "                    entity_type_vs_key_mapping.append(\n",
    "                        (curr_entity_type, (curr_key,)))\n",
    "                    output[curr_key] = {'attribute_name': curr_entity_type}\n",
    "                    output[curr_key]['annotation'] = entity\n",
    "                    output[curr_key]['attribute_type'] = entity_level_map.get(\n",
    "                        output[curr_key]['attribute_name'])\n",
    "                    output[curr_key]['value'] = (\"\" if int(entity[\n",
    "                        'page_no']) == 0 else curr_entity_value)\n",
    "                    output[curr_key]['order'] = entity_order_map.get(\n",
    "                        curr_entity_type)\n",
    "                else:\n",
    "                    dummy_key = 'dummy_' + str(dummy_var_count)\n",
    "                    dummy_entity = create_dummy_entity(\n",
    "                        dummy_entity_name=dummy_key,\n",
    "                        curr_entity_type=curr_entity_type,\n",
    "                        di_rel_graph=di_rel_graph)\n",
    "                    dummy_var_count += 1\n",
    "                    model_output_queue.append(entity)\n",
    "                    model_output_queue.append(dummy_entity)\n",
    "                    entity_type_vs_key_mapping = mapping_loop_over_copy\n",
    "        \n",
    "        output_formatted = []\n",
    "        for key, value in output.items():\n",
    "            output_formatted.append(value)\n",
    "        for d in output_formatted:\n",
    "            recursive_dict_iterator(d, di_rel_graph)\n",
    "        # Fill in the blank entities in the output and sort as per sort order\n",
    "        fill_blank_entities_in_k_graph(\n",
    "            output_formatted, di_rel_graph, entity_level_map,\n",
    "            entity_order_map, root_level_ents)\n",
    "        \n",
    "    else:\n",
    "        output_formatted = []\n",
    "        fill_blank_entities_in_k_graph(\n",
    "            output_formatted, di_rel_graph, entity_level_map,\n",
    "            entity_order_map, root_level_ents)\n",
    "\n",
    "    return output_formatted\n",
    "\n",
    "\n",
    "def create_dummy_entity(dummy_entity_name, curr_entity_type, di_rel_graph):\n",
    "    \"\"\"Create dummy entity for extract_entity_relation function.\"\"\"\n",
    "    dummy_entity_type = tuple(\n",
    "        di_rel_graph.predecessors(curr_entity_type))[0]\n",
    "    dummy_entity = {\n",
    "        \"annotation\": dummy_entity_type,\n",
    "        \"page_no\": \"0\",\n",
    "        \"sentence_no\": \"0\",\n",
    "        \"word\": dummy_entity_name,\n",
    "        \"word_list\": [dummy_entity_name],\n",
    "        \"starting_index\": [0],\n",
    "        \"end_position\": [0],\n",
    "        \"score\": [\"0.00\"],\n",
    "        \"word_index\": [0]\n",
    "    }\n",
    "    return dummy_entity\n",
    "\n",
    "\n",
    "def get_group_mapping(di_rel_graph):\n",
    "    \"\"\"Create node to group/subgroup/local map, given a directed graph.\"\"\"\n",
    "    group_map_from_rel = {}\n",
    "    for node in di_rel_graph.nodes():\n",
    "        predecesspr_count = len(tuple(di_rel_graph.predecessors(node)))\n",
    "        successor_count = len(tuple(di_rel_graph.successors(node)))\n",
    "        if successor_count == 0:\n",
    "            group_map_from_rel[node] = 'local'\n",
    "            continue\n",
    "        if (successor_count > 0) and (predecesspr_count) == 0:\n",
    "            group_map_from_rel[node] = 'group'\n",
    "            continue\n",
    "        group_map_from_rel[node] = 'subgroup'\n",
    "    return group_map_from_rel\n",
    "\n",
    "\n",
    "def recursive_dict_iterator(d_in,di_rel_graph):\n",
    "    \"\"\"Recursively insert attribute_list in dictionaries having hierarchies.\"\"\"\n",
    "    data_keys_appended = [x for x in d_in.keys() if x not in [\n",
    "        'annotation', 'type', 'value', 'attribute_type',\n",
    "        'attribute_name', 'attribute_list', 'order']]  \n",
    "    \n",
    "    data_keys = [x.split('___')[0] for x in data_keys_appended]\n",
    "    \n",
    "    attrib_list = []\n",
    "    \n",
    "    if len(data_keys) > 0:\n",
    "        successor_1 = list(di_rel_graph.successors(d_in['attribute_name']))\n",
    "        successor_1_key = [x for x in data_keys_appended if x.split('___')[1] in successor_1]\n",
    "        for keys in successor_1_key:\n",
    "            if d_in[keys]['attribute_type'] == 'local':\n",
    "                attrib_list.append(d_in[keys])\n",
    "                del d_in[keys]\n",
    "            else:\n",
    "                temp_attrib_list = d_in[keys]\n",
    "                del d_in[keys]\n",
    "                successor_2 = list(di_rel_graph.successors(keys.split('___')[1]))\n",
    "                index = data_keys_appended.index(keys)\n",
    "                subgroup_attrib_list = []\n",
    "                for i in data_keys_appended[index+1:]:\n",
    "                    if d_in[i]['attribute_type'] == 'subgroup' or i.split('___')[1] not in successor_2:\n",
    "                        break\n",
    "                    else:\n",
    "                        subgroup_attrib_list.append(d_in[i])\n",
    "                        del d_in[i]\n",
    "                \n",
    "                temp_attrib_list['attribute_list'] = subgroup_attrib_list\n",
    "                attrib_list.append(temp_attrib_list)\n",
    "                                \n",
    "    d_in['attribute_list'] = attrib_list\n",
    "\n",
    "\n",
    "def create_blank_entity(entity_name, attrib_list_req,\n",
    "                        entity_level_map, entity_order_map):\n",
    "    \"\"\"Create blank entity if not predicted by model.\"\"\"\n",
    "    attribute_type = entity_level_map.get(entity_name, 'global')\n",
    "    ent = {\n",
    "        \"attribute_name\": entity_name,\n",
    "        \"annotation\": {\n",
    "            \"annotation\": entity_name,\n",
    "            \"word\": \"\",\n",
    "            \"word_index\": [],\n",
    "            \"word_list\": [],\n",
    "            \"starting_index\": [],\n",
    "            \"sentence_no\": \"0\",\n",
    "            \"score\": \"0.00\",\n",
    "            \"end_position\": [],\n",
    "            \"page_no\": \"0\"\n",
    "        },\n",
    "        \"attribute_type\": attribute_type,\n",
    "        \"value\": \"\",\n",
    "        \"order\": entity_order_map.get(entity_name)}\n",
    "    if attrib_list_req:\n",
    "        ent['attribute_list'] = []\n",
    "    return ent\n",
    "\n",
    "\n",
    "def create_blank_hierarchy(entity_name, di_rel_graph,\n",
    "                           entity_level_map, entity_order_map):\n",
    "    \"\"\"Create blank hierarchy for knowledge graph for missing entities.\"\"\"\n",
    "    child_nodes = set()\n",
    "    if entity_name in di_rel_graph.node:\n",
    "        child_nodes = set(di_rel_graph.successors(entity_name))\n",
    "    attrib_list_req = True if len(child_nodes) > 0 else False\n",
    "    ent = create_blank_entity(entity_name, attrib_list_req,\n",
    "                              entity_level_map, entity_order_map)\n",
    "    if len(child_nodes) > 0:\n",
    "        for child_node in child_nodes:\n",
    "            ent['attribute_list'].append(\n",
    "                create_blank_hierarchy(child_node, di_rel_graph,\n",
    "                                       entity_level_map, entity_order_map)\n",
    "            )\n",
    "    return ent\n",
    "\n",
    "\n",
    "def fill_blank_entities_in_k_graph(graph_output, di_rel_graph,\n",
    "                                   entity_level_map, entity_order_map,\n",
    "                                   root_level_ents):\n",
    "    \"\"\"Format the knowledge graph output as per the document definition.\"\"\"\n",
    "    ent_types_found = set(x['attribute_name'] for x in graph_output)\n",
    "    for ent in root_level_ents:\n",
    "        if ent not in ent_types_found:\n",
    "            graph_output.append(\n",
    "                create_blank_hierarchy(ent, di_rel_graph,\n",
    "                                       entity_level_map, entity_order_map)\n",
    "            )\n",
    "    graph_output.sort(key=lambda x: int(x['order']))\n",
    "    for entity in graph_output:\n",
    "        check_and_fill_blank_entities(\n",
    "            entity, di_rel_graph, entity_level_map, entity_order_map)\n",
    "    #added logic to sort on the basis of score of prediction in descending order\n",
    "    graph_output.sort(key=lambda x: (int(x['order']),x['annotation']['score']),\n",
    "                      reverse=True)\n",
    "    graph_output.sort(key=lambda x: int(x['order']))\n",
    "\n",
    "\n",
    "def check_and_fill_blank_entities(entity, di_rel_graph,\n",
    "                                  entity_level_map, entity_order_map):\n",
    "    \"\"\"Fill in blank entities as per the hierarchy defined in graph.\"\"\"\n",
    "    entity_type = entity['attribute_name']\n",
    "    if entity['attribute_type'] not in ('group', 'subgroup'):\n",
    "        return\n",
    "\n",
    "    attribute_list = entity.get('attribute_list', [])\n",
    "    child_nodes_found = set((x['attribute_name'] for x in attribute_list))\n",
    "    child_nodes = set(di_rel_graph.successors(entity_type))\n",
    "    missing_nodes = child_nodes - child_nodes_found\n",
    "\n",
    "    for missing_node in missing_nodes:\n",
    "        attribute_list.append(\n",
    "            create_blank_hierarchy(missing_node, di_rel_graph,\n",
    "                                   entity_level_map, entity_order_map))\n",
    "\n",
    "    if len(missing_nodes) > 0:\n",
    "        for attribute in attribute_list:\n",
    "            if attribute['attribute_type'] == 'subgroup':\n",
    "                attribute['attribute_list'].sort(key=lambda x: (int(x['order']),\n",
    "                         x['annotation']['score']), reverse=True)\n",
    "                attribute['attribute_list'].sort(key=lambda x: int(x['order']))\n",
    "                \n",
    "        entity['attribute_list'] = attribute_list\n",
    "\n",
    "    if len(attribute_list) > 0:\n",
    "        attribute_list.sort(key=lambda x: (int(x['order']), x['annotation']['score']),\n",
    "                            reverse=True)\n",
    "        attribute_list.sort(key=lambda x: int(x['order']))\n",
    "\n",
    "    # check for existing entities not included in above step\n",
    "    for existing_entity in attribute_list:\n",
    "        existing_ent_type = existing_entity['attribute_name']\n",
    "        if existing_ent_type not in missing_nodes:\n",
    "            check_and_fill_blank_entities(\n",
    "                existing_entity, di_rel_graph,\n",
    "                entity_level_map, entity_order_map)\n",
    "\n",
    "        \n",
    "def get_model_predictions_for_npz(model, npz_image_path, class_encoder):\n",
    "    pixels = np.load(npz_image_path)['pixels']\n",
    "    predicted_labels = model.predict(np.array([pixels]), batch_size=1)\n",
    "    return class_encoder.one_hot_decode(predicted_labels[0].astype(np.float64))\n",
    "\n",
    "    \n",
    "def get_inference_IC_run(model, input_img_path,class_encoder):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function takes model_id and image path as input and predict class of\n",
    "    the image.\n",
    "    \n",
    "     Args:\n",
    "        input_img_type     - Type of Image i.e. PNG, JPEG, JPEG\n",
    "                                 has token, char and label\n",
    "        input_img_path     - path of image\n",
    "        model_id           - model_id of the trained model\n",
    "        \n",
    "    Returns:\n",
    "        Prediction of the class and its prediction accuracy percentage !   \n",
    "    \"\"\"\n",
    "    #Load Trained Models\n",
    "    try:\n",
    "\n",
    "        for npz_image_path in list(glob.glob(input_img_path)):\n",
    "            class_name = 'unknown'\n",
    "            character_name_to_probability = get_model_predictions_for_npz(model,\n",
    "                                                                          npz_image_path,\n",
    "                                                                          class_encoder)\n",
    "            img_name = os.path.basename(npz_image_path)\n",
    "            for clas, prob in character_name_to_probability.items():\n",
    "                if prob >  image_config['IC_PRED_PROB_THRESHOLD']:\n",
    "                    class_name = clas\n",
    "                    probabolity = round(prob,2)\n",
    "\n",
    "            #print (img_name ,\" = \", class_name, '\\n',\"  predi_prob = \", probabolity)\n",
    "            print(\"Inferene Generated for Iamge : '{}'\".format(img_name))\n",
    "            return json.dumps({\"status\": \"success\",\n",
    "                               \"image_name\": str(img_name),\n",
    "                               \"class_name\": class_name,\n",
    "                               \"probabolity_percent\": probabolity})\n",
    "    except Exception as exception:\n",
    "        print(exception)\n",
    "        print(\"Inferene Failed, Try other Image or Contact Admin..!\")\n",
    "        return json.dumps({\"status\": \"failure\", \"result\": str(exception)})\n",
    "    \n",
    "def get_zoning_model(model_file_path):\n",
    "    tf.reset_default_graph()\n",
    "    graph = tf.get_default_graph()\n",
    "    with graph.as_default():\n",
    "        od_graph_def = tf.GraphDef()\n",
    "        with tf.gfile.GFile(model_file_path, 'rb') as fid:\n",
    "            serialized_graph = fid.read()\n",
    "            od_graph_def.ParseFromString(serialized_graph)\n",
    "            tf.import_graph_def(od_graph_def, name='')\n",
    "\n",
    "    with graph.as_default():\n",
    "        model= graph\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Ensemble_ModelDetails' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-69fbb1b7841a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# and same being defined as global in score function for model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEnsemble_ModelDetails\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCONFIG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'MODEL_BASE_LOCATION'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainModel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"single model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Ensemble_ModelDetails' is not defined"
     ]
    }
   ],
   "source": [
    "# Suporting files of model created on local pod during training are loaded to variable,\n",
    "# and same being defined as global in score function for model \n",
    "\n",
    "if len(Ensemble_ModelDetails)==0:\n",
    "    model_path = os.path.join(CONFIG['MODEL_BASE_LOCATION'], TrainModel['model_id'])\n",
    "    print(\"Loading supporting files for Non Ensemble model\")\n",
    "    if 'NER' in model_type and CCFlag == 'False':\n",
    "        word2idx = pkl.load(open(model_path+'/'+'word2Idx.pkl','rb'))\n",
    "        label2idx = pkl.load(open(model_path+'/'+'label2Idx.pkl','rb'))\n",
    "        case2idx = pkl.load(open(model_path+'/'+'case2Idx.pkl','rb'))\n",
    "        char2idx = pkl.load(open(model_path+'/'+'char2Idx.pkl','rb'))\n",
    "        valid_char_set = pkl.load(open(model_path+'/'+'valid_char_set.pkl','rb'))\n",
    "        model_file_path = model_path+'/valid_char_set.pkl'\n",
    "        model = keras_load_model(model_path+'/'+\"model_complete.h5\")\n",
    "        print(\"NER model path - \",model_path)\n",
    "\n",
    "    if 'NER' in model_type and CCFlag == 'True':\n",
    "        word2idx = pkl.load(open(model_path+'/'+'word2Idx.pkl','rb'))\n",
    "        label2idx = pkl.load(open(model_path+'/'+'label2Idx.pkl','rb'))\n",
    "        case2idx = pkl.load(open(model_path+'/'+'case2Idx.pkl','rb'))\n",
    "        char2idx = pkl.load(open(model_path+'/'+'char2Idx.pkl','rb'))\n",
    "        valid_char_set = pkl.load(open(model_path+'/'+'valid_char_set.pkl','rb'))\n",
    "        svc_clf_file_name = '%s_content_classification_model.pkl' % TrainModel['model_id']\n",
    "        tf_idf_model_file_name = '%s_content_classification_tf_idf_model.pkl' % TrainModel['model_id']\n",
    "        svc_clf = pkl.load(open(model_path+'/'+svc_clf_file_name,'rb'))\n",
    "        tf_idf_model = pkl.load(open(model_path+'/'+tf_idf_model_file_name,'rb'))\n",
    "        model_file_path = model_path+'/valid_char_set.pkl'\n",
    "        model = keras_load_model(model_path+'/'+\"model_complete.h5\")\n",
    "        print(\"svc_clf - \",model_path+'/'+svc_clf_file_name)\n",
    "        print(\"tf_idf_model - \",model_path+'/'+tf_idf_model_file_name)\n",
    "\n",
    "    if 'DC' in model_type:\n",
    "        svc_clf_file_name = '%s_document_classification_svc_model.pkl' % TrainModel['model_id']\n",
    "        tf_idf_model_file_name = '%s_document_classification_tf_idf_model.pkl' % TrainModel['model_id']\n",
    "        input_data_file_name = '%s_document_classification_input_data.pkl' % TrainModel['model_id']\n",
    "        svc_clf = pkl.load(open(model_path+'/'+svc_clf_file_name,'rb'))\n",
    "        tf_idf_model = pkl.load(open(model_path+'/'+tf_idf_model_file_name,'rb'))\n",
    "        dc_input_data = pkl.load(open(model_path+'/'+input_data_file_name,'rb'))\n",
    "        model_file_path = model_path+'/'+svc_clf_file_name\n",
    "        model = tf_idf_model\n",
    "        print(\"svc_clf - \",model_path+'/'+svc_clf_file_name)\n",
    "        print(\"tf_idf_model - \",model_path+'/'+tf_idf_model_file_name)\n",
    "        print(\"dc_input_data - \",model_path+'/'+input_data_file_name)\n",
    "\n",
    "    if 'IC' in model_type:\n",
    "        class_encoder = pkl.load(open(model_path+'/'+'image_classes_encoder.pkl','rb'))\n",
    "        model_file_path = model_path+'/model_complete.h5'\n",
    "        model = keras_load_model(model_path+'/'+\"model_complete.h5\")\n",
    "        print(\"class_encoder - \",model_path+'/'+'image_classes_encoder.pkl')\n",
    "\n",
    "    if 'Object Identification' in model_type :\n",
    "        zoning_file_label_path = model_path+'/'+'label_map.pbtxt'\n",
    "        label_map = label_map_util.load_labelmap(zoning_file_label_path)\n",
    "        num_classes = len(label_map.item)\n",
    "        categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=num_classes, use_display_name=True)\n",
    "        category_index = label_map_util.create_category_index(categories)\n",
    "        model_file_path = model_path+'/frozen_inference_graph.pb'\n",
    "        model = get_zoning_model(model_file_path)\n",
    "        # to get OI image rescale flag\n",
    "        img_rescale_flag = get_OI_rescale_flag(solution_id,files,new_model_id, created_by)\n",
    "        print(\"img_rescale_flag and updating metadata - \",img_rescale_flag)\n",
    "        try:\n",
    "            OI_MultiCropFlag = multiple_crop_zone[0]['multi_crop_flag']\n",
    "        except:\n",
    "            OI_MultiCropFlag = True\n",
    "        print(\" OI_MultiCropFlag updated \")    \n",
    "        update_mosaicml_metadata_info(new_model_id,data={'img_rescale_flag':img_rescale_flag,'OI_MultiCropFlag':OI_MultiCropFlag,'zone_config':zone_config})\n",
    "        print(\"zoning_file_label_path - \",zoning_file_label_path)\n",
    "        print(\"category_index - \",category_index)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Files transfer to s3 this is not used any more will be deleted.\n",
    "if OnPremiseFlag!=True:\n",
    "    \n",
    "    import time\n",
    "    #time.sleep(30)\n",
    "    import boto3\n",
    "    from EEF_NER_LSTM_Training.S3_module import conn ,bucket, s3\n",
    "\n",
    "    def upload_file(file_name, bucket, folder_name=None):\n",
    "        \"\"\"Upload a file to an S3 bucket\n",
    "\n",
    "        :param file_name: File to upload\n",
    "        :param bucket: Bucket to upload to\n",
    "        :param folder_name: upload to specific folder\n",
    "        :return: True if file was uploaded, else False\n",
    "        \"\"\"\n",
    "\n",
    "        # Upload the file\n",
    "        if folder_name:\n",
    "            #response = s3_client.upload_file(file_name, bucket, folder_name+'/'+file_name.split('/')[-1])\n",
    "            response = s3.upload_file(file_name, bucket, folder_name+'/'+file_name.split('/')[-1])\n",
    "        else:\n",
    "            #response = s3_client.upload_file(file_name, bucket, file_name)\n",
    "            response = s3.upload_file(file_name, bucket, file_name)\n",
    "        return True\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @scoring_func\n",
    "def predict_ner_CC(model,request):\n",
    "    global word2idx, case2idx, char2idx, label2idx, tf_idf_model, svc_clf, language\n",
    "    print(\"Score Function language used for Predection \",language)\n",
    "    import nltk\n",
    "    nltk.data.path.append('/data/NER/nltk_data/')\n",
    "    payload = request.json[\"payload\"]\n",
    "    request_data = payload\n",
    "    text = request_data['text']\n",
    "    prediction_format = request_data['prediction_format']\n",
    "    k_graph = request_data.get('knowledge_graph')\n",
    "    entity_order_map = request_data.get('entity_order_map')\n",
    "    root_level_ents = request_data.get('root_level_ents')\n",
    "\n",
    "    result = get_inference_run_CC(text, tf_idf_model, svc_clf, prediction_format, model, k_graph, entity_order_map, root_level_ents,language)\n",
    "    return result\n",
    "\n",
    "def predict_ner(model,request):\n",
    "    global word2idx, case2idx, char2idx, label2idx, language \n",
    "    print(\"Score Function language used for Predection \",language)\n",
    "    import nltk\n",
    "    nltk.data.path.append('/data/NER/nltk_data/')\n",
    "    payload = request.json[\"payload\"]\n",
    "    request_data = payload\n",
    "    text = request_data['text']\n",
    "    prediction_format = request_data['prediction_format']\n",
    "    k_graph = request_data.get('knowledge_graph')\n",
    "    entity_order_map = request_data.get('entity_order_map')\n",
    "    root_level_ents = request_data.get('root_level_ents')\n",
    "\n",
    "    result = get_inference_run(text, prediction_format, model, language, k_graph, entity_order_map, root_level_ents, language)\n",
    "    return result\n",
    "\n",
    "def predict_Doc_classification(model,request):\n",
    "    global svc_clf, dc_input_data\n",
    "    type(dc_input_data)\n",
    "    payload = request.json[\"payload\"]\n",
    "    request_data = payload\n",
    "    text = request_data['text']['file_data']\n",
    "    transformed_vector_text = model.transform([text])\n",
    "    predicted_proba = svc_clf.predict_proba(transformed_vector_text)\n",
    "    class_pred = svc_clf.predict(transformed_vector_text)\n",
    "    recommendation = 'unknown'\n",
    "    \n",
    "    prob_score = 0.00\n",
    "    for i, proba_score in enumerate(predicted_proba):\n",
    "        prob_score = max(proba_score)     \n",
    "\n",
    "    if prob_score > float(CONFIG['DC_PRED_PROB_THRESHOLD']):\n",
    "         recommendation = class_pred[0]\n",
    "    \n",
    "    result = {'class':class_pred[0], 'probability':str(prob_score), 'recommended_class':recommendation}\n",
    "    return result\n",
    "\n",
    "def predict_IC(model,request):\n",
    "    import os\n",
    "    import sys\n",
    "    import uuid\n",
    "    sys.path.insert(0,\"/home/mosaic-ai/temp/notebooks/EEF_NER_LSTM_Training\")\n",
    "    sys.path.insert(1,\"/home/mosaic-ai/temp/notebooks/EEF_NER_LSTM_Training/services\")\n",
    "    from EEF_NER_LSTM_Training.utility import utility_application as app_util\n",
    "    from EEF_NER_LSTM_Training.cloud_utility import download_file_object_from_cloud_storage\n",
    "    CONFIG = app_util.get_system_config()[1]\n",
    "    OnPremiseFlag = CONFIG['OnPremiseFlag']\n",
    "    global class_encoder\n",
    "    \n",
    "\n",
    "    try:\n",
    "        payload = request.json[\"payload\"]\n",
    "        request_data = payload\n",
    "        input_img_path = request_data['input_img_path']\n",
    "        print(\"Input Image Path \", input_img_path)\n",
    "        input_img_type = request_data['input_img_type']\n",
    "        \n",
    "        # added for cloud storage\n",
    "        try:\n",
    "            os.makedirs(CONFIG['IC_DATA_INF_PATH'], exist_ok=True)\n",
    "            print(\" dir created \", CONFIG['IC_DATA_INF_PATH'])\n",
    "        except BaseException as e:\n",
    "            print(\" IC folder already exists \",e)\n",
    "            pass            \n",
    "\n",
    "        curr_dir = os.getcwd()\n",
    "        image_parent_path = os.path.dirname(input_img_path)\n",
    "        \n",
    "        if OnPremiseFlag != True:\n",
    "            download_folder_name_connector_pvc = str(uuid.uuid1())\n",
    "            folder_path = download_file_object_from_cloud_storage(CONFIG['STORAGE_AT'], input_img_path,\n",
    "                                                                  download_folder_name_connector_pvc,\n",
    "                                                                  temp_folder=os.path.join(CONFIG['IC_DATA_INF_PATH'],\n",
    "                                                                                           os.path.dirname(input_img_path)))\n",
    "\n",
    "        else:\n",
    "            os.chdir(CONFIG['IC_DATA_INF_PATH'])\n",
    "            print(\" In on-premise PV storage\")\n",
    "            os.makedirs(os.path.join(CONFIG['IC_DATA_INF_PATH'], image_parent_path),exist_ok=True)\n",
    "            fp = os.path.join(CONFIG['IC_DATA_INF_PATH'], input_img_path)\n",
    "            print(\"File path for PV \",fp,\"input_img_path\",input_img_path)\n",
    "            try:\n",
    "                shutil.copy(input_img_path, fp)\n",
    "            except BaseException as e:\n",
    "                print(\"Unable to copy IC files to PV storage\",e)\n",
    "                                 \n",
    "        os.chdir(curr_dir)\n",
    "                                    \n",
    "        if OnPremiseFlag != True:\n",
    "            input_img_path = os.path.join(CONFIG['IC_DATA_INF_PATH'], input_img_path)\n",
    "        else:\n",
    "            input_img_path = os.path.join(CONFIG['IC_DATA_INF_PATH'], input_img_path)\n",
    "            print(\"PV storage image path \",input_img_path)\n",
    "        \n",
    "        input_images_data = list(glob.glob(input_img_path))\n",
    "        \n",
    "        #preprocess image and convert to npz format\n",
    "        img_prepro.image_preprocessing(input_images_data)\n",
    "        image_basepath, _ = os.path.splitext(input_img_path)\n",
    "        input_img_path = image_basepath+'.npz'\n",
    "\n",
    "        result = get_inference_IC_run(model, input_img_path, class_encoder)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "\n",
    "def predict_zoning_inference(model,request):\n",
    "    import os\n",
    "    import sys\n",
    "    import uuid\n",
    "    from pathlib import Path\n",
    "    \n",
    "    sys.path.insert(0,\"/home/mosaic-ai/temp/notebooks/EEF_NER_LSTM_Training\")\n",
    "    sys.path.insert(1,\"/home/mosaic-ai/temp/notebooks/EEF_NER_LSTM_Training/services\")\n",
    "\n",
    "    from EEF_NER_LSTM_Training.utility import utility_application as app_util\n",
    "    from EEF_NER_LSTM_Training.cloud_utility import download_file_object_from_cloud_storage, upload_file_obj_to_cloud_storage\n",
    "    CONFIG = app_util.get_system_config()[1]\n",
    "    OnPremiseFlag = CONFIG['OnPremiseFlag']        \n",
    "    global category_index\n",
    "    \"\"\"\n",
    "    Route to call the inference of the model.\n",
    "\n",
    "    Keyword arguments:\n",
    "    model_id    - selects the models which will be used to infer\n",
    "    image_path  - path of input image\n",
    "    \"\"\"\n",
    "    # added since POD is not able to get the EEF module\n",
    "        \n",
    "    try:\n",
    "        payload = request.json[\"payload\"]\n",
    "        request_data = payload\n",
    "        image_path = request_data['image_path']\n",
    "        print(\"Input Image Path \",image_path)\n",
    "        image_parent_path = os.path.dirname(image_path)\n",
    "\n",
    "        # added for cloud storage\n",
    "        try:\n",
    "            os.makedirs(CONFIG['OI_DATA_INF_PATH'], exist_ok=True)\n",
    "            print(\" dir created \", CONFIG['OI_DATA_INF_PATH'])\n",
    "        except BaseException as e:\n",
    "            print(\" OI Inference folder already exists \",e)\n",
    "            pass            \n",
    "        curr_dir = os.getcwd()\n",
    "\n",
    "        if OnPremiseFlag!=True:\n",
    "            download_folder_name_connector_pvc = str(uuid.uuid1())\n",
    "            folder_path = download_file_object_from_cloud_storage(CONFIG['STORAGE_AT'], image_path,\n",
    "                                                                  download_folder_name_connector_pvc,\n",
    "                                                                  temp_folder=os.path.join(CONFIG['OI_DATA_INF_PATH'],\n",
    "                                                                                           os.path.dirname(image_path)))\n",
    "\n",
    "        else:\n",
    "            os.chdir(CONFIG['OI_DATA_INF_PATH'])\n",
    "            if not os.path.exists(os.path.join(CONFIG['OI_DATA_INF_PATH'], image_parent_path)):\n",
    "                os.makedirs(os.path.join(CONFIG['OI_DATA_INF_PATH'], image_parent_path))\n",
    "                print(\"dir created\",CONFIG['OI_DATA_INF_PATH'], image_parent_path)\n",
    "            try:\n",
    "                fp =  os.path.join(CONFIG['OI_DATA_INF_PATH'], image_path)\n",
    "                shutil.copy(image_path,fp)\n",
    "            except BaseException as e:\n",
    "                print(\"fail to copy OI files to PV storage\",e)        \n",
    "                                         \n",
    "        os.chdir(curr_dir)\n",
    "        if OnPremiseFlag!=True:            \n",
    "            image_path = os.path.join(CONFIG['OI_DATA_INF_PATH'], image_path)\n",
    "            print(\"full image path.................>\",image_path)\n",
    "        else:\n",
    "            image_path = os.path.join(CONFIG['OI_DATA_INF_PATH'], image_path)\n",
    "            print(\"PV full image path.................>\",image_path)                                    \n",
    "\n",
    "        \n",
    "        result = infer_zoning.run(image_path, model, category_index)\n",
    "        print(\"result\",result)\n",
    "        \n",
    "        if len (result)>0:\n",
    "            for i in range(len(result)):\n",
    "                file_name = result[i]['path']\n",
    "#                 print(\"OI File name \",file_name)\n",
    "                folder_path = Path(request_data['image_path'])\n",
    "                folder_name = str(folder_path.parent)\n",
    "                print(\"OI Folder name \",folder_name,type(folder_name))\n",
    "                if OnPremiseFlag!=True:\n",
    "                    if folder_name:\n",
    "                        code, local_path = upload_file_obj_to_cloud_storage(CONFIG['STORAGE_AT'], open(file_name, 'rb'),\n",
    "                                                                            folder_name+'/'+file_name.split('/')[-1])\n",
    "                    else:\n",
    "                        code, local_path = upload_file_obj_to_cloud_storage(CONFIG['STORAGE_AT'], open(file_name, 'rb'),\n",
    "                                                                            file_name)\n",
    "                else:\n",
    "                    shutil.copy(file_name, folder_name+'/'+file_name.split('/')[-1])                                          \n",
    "        return result\n",
    "    except Exception as exception:\n",
    "        return print(str(exception))    \n",
    "\n",
    "def predict_zoning_inference_new(model,request):\n",
    "    import sys\n",
    "    import os\n",
    "    import uuid\n",
    "    \n",
    "    sys.path.insert(0,\"/home/mosaic-ai/temp/notebooks/EEF_NER_LSTM_Training\")\n",
    "    sys.path.insert(1,\"/home/mosaic-ai/temp/notebooks/EEF_NER_LSTM_Training/services\")\n",
    "    \n",
    "    from pathlib import Path\n",
    "    from EEF_NER_LSTM_Training.utility import utility_application as app_util\n",
    "    from EEF_NER_LSTM_Training.cloud_utility import download_file_object_from_cloud_storage, upload_file_obj_to_cloud_storage\n",
    "    CONFIG = app_util.get_system_config()[1]\n",
    "    OnPremiseFlag = CONFIG['OnPremiseFlag']        \n",
    "    global category_index,img_rescale_flag,OI_MultiCropFlag,zone_config\n",
    "    \"\"\"\n",
    "    Route to call the inference of the model.\n",
    "\n",
    "    Keyword arguments:\n",
    "    model_id    - selects the models which will be used to infer\n",
    "    image_path  - path of input image\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        payload = request.json[\"payload\"]\n",
    "        request_data = payload\n",
    "        image_path = request_data['image_path']\n",
    "        print(\"Input image path\", image_path)\n",
    "        image_parent_path = os.path.dirname(image_path)\n",
    "\n",
    "        # added for cloud storage\n",
    "        try:\n",
    "            os.makedirs(CONFIG['OI_DATA_INF_PATH'], exist_ok=True)\n",
    "            print(\"OI Inference Folder created \", CONFIG['OI_DATA_INF_PATH'])\n",
    "        except BaseException as e:\n",
    "            print(\" OI Inference Folder already exists\", e)\n",
    "            pass            \n",
    "        curr_dir = os.getcwd()\n",
    "\n",
    "        if OnPremiseFlag!=True:\n",
    "            download_folder_name_connector_pvc = str(uuid.uuid1())\n",
    "            folder_path = download_file_object_from_cloud_storage(CONFIG['STORAGE_AT'], image_path,\n",
    "                                                                  download_folder_name_connector_pvc,\n",
    "                                                                  temp_folder=os.path.join(CONFIG['OI_DATA_INF_PATH'],\n",
    "                                                                                           os.path.dirname(image_path)))\n",
    "        else:\n",
    "            os.chdir(CONFIG['OI_DATA_INF_PATH'])\n",
    "            if not os.path.exists(os.path.join(CONFIG['OI_DATA_INF_PATH'], image_parent_path)):\n",
    "                os.makedirs(os.path.join(CONFIG['OI_DATA_INF_PATH'], image_parent_path))\n",
    "                print(\"dir created\", CONFIG['OI_DATA_INF_PATH'],  image_parent_path)\n",
    "            try:\n",
    "                fp =  os.path.join(CONFIG['OI_DATA_INF_PATH'], image_path)\n",
    "                shutil.copy(image_path,fp)\n",
    "            except BaseException as e:\n",
    "                print(\"fail to copy OI files to PV storage\",e)        \n",
    "                                         \n",
    "        os.chdir(curr_dir)\n",
    "        if OnPremiseFlag!=True:            \n",
    "            image_path = os.path.join(CONFIG['OI_DATA_INF_PATH'], image_path)\n",
    "            print(\"Image path \", image_path)\n",
    "        else:\n",
    "            image_path = os.path.join(CONFIG['OI_DATA_INF_PATH'], image_path)\n",
    "            print(\"PV Image path\", image_path)                                    \n",
    "\n",
    "        result = infer_zoning.run_new_pdf(image_path, model, category_index,img_rescale_flag,OI_MultiCropFlag,zone_config)\n",
    "        \n",
    "#         print(\"result\",result)\n",
    "        \n",
    "        if len (result)>0:\n",
    "            for i in range(len(result)):\n",
    "                file_name = result[i]['path']\n",
    "#                 print(\"OI File name \",file_name)\n",
    "                folder_path = Path(request_data['image_path'])\n",
    "                folder_name = str(folder_path.parent)\n",
    "                print(\"OI Folder name \",folder_name,type(folder_name))\n",
    "                \n",
    "                if OnPremiseFlag!=True:\n",
    "                    if folder_name:\n",
    "                        code, local_path = upload_file_obj_to_cloud_storage(CONFIG['STORAGE_AT'], open(file_name, 'rb'),\n",
    "                                                                            folder_name+'/'+file_name.split('/')[-1])\n",
    "                    else:\n",
    "                        code, local_path = upload_file_obj_to_cloud_storage(CONFIG['STORAGE_AT'], open(file_name, 'rb'),\n",
    "                                                                            file_name)\n",
    "                else:\n",
    "                    try:\n",
    "                        shutil.copy(file_name, folder_name+'/'+file_name.split('/')[-1])\n",
    "                    except:\n",
    "                        pass\n",
    "                        print(\"file already exits\")\n",
    "                                                              \n",
    "        return result\n",
    "    except Exception as exception:\n",
    "        return print(str(exception))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Files paths to upload to cloud\n",
    "#Folder_name = TrainModel['model_id']\n",
    "#files = os.listdir(os.path.join(CONFIG['MODEL_BASE_LOCATION'], Folder_name))\n",
    "\n",
    "#print(\"Folder_name - \",Folder_name)\n",
    "#print(\"Model files - \",files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(Ensemble_ModelDetails)==0:\n",
    "    ml_model_id = TrainModel['model_id'].split(\"__\")[0]\n",
    "    version_id = TrainModel['model_id'].split(\"__\")[1]\n",
    "    print(\"Updating Trained Model\")\n",
    "    print(\"ml_model_id - \", ml_model_id)\n",
    "    print(\"version_id - \", version_id)\n",
    "    model_obj = model\n",
    "    \n",
    "    if 'NER' in model_type and CCFlag == 'True':    \n",
    "        UpdateModel = update_existing_model(ml_model_id, version_id, model_obj, scoring_func = predict_ner_CC, flavour = MLModelFlavours.keras, init_script=None, schema=None, metadata_info=None, input_type='json', target_names=None, datasource_name=None)\n",
    "        print(\"Updated NER-CC Model\")\n",
    "\n",
    "    if 'NER' in model_type and CCFlag == 'False':    \n",
    "        UpdateModel = update_existing_model(ml_model_id, version_id, model_obj, scoring_func = predict_ner, flavour = MLModelFlavours.keras, init_script=None, schema=None, metadata_info=None, input_type='json', target_names=None, datasource_name=None)\n",
    "        print(\"Updated NER Model\")\n",
    "\n",
    "    if 'DC' in model_type :\n",
    "        UpdateModel = update_existing_model(ml_model_id, version_id, model_obj, scoring_func = predict_Doc_classification, flavour = MLModelFlavours.sklearn, init_script=None, schema=None, metadata_info=None, input_type='json', target_names=None, datasource_name=None)\n",
    "        print(\"Updated Document Classification Model\")\n",
    "\n",
    "    if 'IC' in model_type :\n",
    "        UpdateModel = update_existing_model(ml_model_id, version_id, model_obj, scoring_func = predict_IC, flavour = MLModelFlavours.keras, init_script=\"mkdir temp \\\\n git clone https://ailogtest:zmVXyxj5yNb97KztBEc_@git.lti-aiq.in/ailogtest/62641a92-ae89-4bb9-86b9-173a01d36618.git temp/ \\\\n export PYTHONPATH=/home/mosaic-ai/temp/notebooks/EEF_NER_LSTM_Training:$PYTHONPATH \\\\n cp -r /home/mosaic-ai/temp/notebooks/EEF_NER_LSTM_Training /home/mosaic-ai/.local/lib/python3.7/site-packages/ \\\\n\"  , schema=None, metadata_info=None, input_type='json', target_names=None, datasource_name=None)\n",
    "        print(\"Updated Image Classification Model\")\n",
    "\n",
    "    if 'Object Identification' in model_type :\n",
    "        UpdateModel = update_existing_model(ml_model_id, version_id, model_obj, scoring_func = predict_zoning_inference_new, flavour = MLModelFlavours.tensorflow, init_script=\"mkdir temp \\\\n git clone https://ailogtest:zmVXyxj5yNb97KztBEc_@git.lti-aiq.in/ailogtest/62641a92-ae89-4bb9-86b9-173a01d36618.git temp/ \\\\n export PYTHONPATH=/home/mosaic-ai/temp/notebooks/EEF_NER_LSTM_Training:$PYTHONPATH \\\\n cp -r /home/mosaic-ai/temp/notebooks/EEF_NER_LSTM_Training /home/mosaic-ai/.local/lib/python3.7/site-packages/ \\\\n\" , schema=None, metadata_info=None, input_type='json', target_names=None, datasource_name=None)\n",
    "        print(\"Updated Object Identification Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update Trained Model Serving Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_serving_image(env, docker_image_url, project_id, model_id, version_id):\n",
    "    \"\"\"\"send put request to url with image.\"\"\"\n",
    "    \"\"\"\n",
    "    env : can be QA /GA /Cloud\n",
    "    docker_image_url = keras or pytorch\n",
    "    \"\"\"\n",
    "    \n",
    "    payload = {\"docker_image_url\":docker_image_url} \n",
    "    \n",
    "    api = \"/registry/api/v1/ml-model/{model_id}/version/{version_id}\"\n",
    "    \n",
    "    \n",
    "    env_url = env.lower() + api\n",
    "    url = env_url.format(model_id=model_id,version_id=version_id)\n",
    "    print(\"Inside get_current_serving_image\")\n",
    "    print(f\"url: {url}\\ndata: {json.dumps(payload)}\\nheaders: {req_header}\")\n",
    "\n",
    "    req_response = requests.get(url, data=json.dumps(payload), headers=req_header, verify = False)\n",
    "    print(f\"req_response: {req_response}\")\n",
    "    \n",
    "    return req_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_serving_image(env, docker_image_url, project_id, model_id, version_id):\n",
    "    \"\"\"\"send put request to url with image.\"\"\"\n",
    "    \"\"\"\n",
    "    env : can be QA /GA /Cloud\n",
    "    docker_image_url = keras or pytorch\n",
    "    \"\"\"\n",
    "    \n",
    "    payload = {\"docker_image_url\":docker_image_url}\n",
    "    \n",
    "    api = \"/registry/api/v1/ml-model/{model_id}/version/{version_id}\"\n",
    "    \n",
    "    \n",
    "    env_url = env.lower() + api\n",
    "    url = env_url.format(model_id=model_id,version_id=version_id)\n",
    "    print(\"Serving model image URL - \", url)\n",
    "    \n",
    "    req_response = requests.put(url, data=json.dumps(payload), headers=req_header, verify = False)\n",
    "    \n",
    "    return req_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"pip install --user -i https://pypi.org/simple polyglot\\n pip install --user -i https://pypi.org/simple pyicu\\n pip install --user -i https://pypi.org/simple pycld2\\n pip install --user -i https://pypi.org/simple morfessor\\n \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_old_models(env, docker_image_url, project_id, model_id, version_id):\n",
    "    \"\"\"\"send put request to url for old models to update image and removes the init_script\"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    env : can be QA /GA /Cloud\n",
    "    docker_image_url = keras or pytorch\n",
    "    \"\"\"\n",
    "    payload = {\"docker_image_url\":docker_image_url, \"init_script\": \"\"}\n",
    "#     if language==\"mongolian\":\n",
    "#         payload = {\"docker_image_url\":docker_image_url, \"init_script\": \"InBpcCBpbnN0YWxsIC0tdXNlciAtaSBodHRwczovL3B5cGkub3JnL3NpbXBsZSBwb2x5Z2xvdFxuIHBpcCBpbnN0YWxsIC0tdXNlciAtaSBodHRwczovL3B5cGkub3JnL3NpbXBsZSBweWljdVxuIHBpcCBpbnN0YWxsIC0tdXNlciAtaSBodHRwczovL3B5cGkub3JnL3NpbXBsZSBweWNsZDJcbiBwaXAgaW5zdGFsbCAtLXVzZXIgLWkgaHR0cHM6Ly9weXBpLm9yZy9zaW1wbGUgbW9yZmVzc29yXG4gIg==\"}\n",
    "#     else:\n",
    "#         payload = {\"docker_image_url\":docker_image_url, \"init_script\": \"\"}\n",
    "    \n",
    "    api = \"/registry/api/v1/ml-model/{model_id}/version/{version_id}\"\n",
    "    \n",
    "    env_url = env.lower() + api\n",
    "    url = env_url.format(model_id=model_id,version_id=version_id)\n",
    "    print(\"Serving old model image URL - \", url)\n",
    "\n",
    "    req_response = requests.put(url, data=json.dumps(payload), headers=req_header, verify = False)\n",
    "    \n",
    "    return req_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = app_util.get_system_config()[1]\n",
    "project_id = CONFIG['project_id']\n",
    "user_name = created_by\n",
    "user_email = created_by\n",
    "static_serving_image = CONFIG['keras']\n",
    "update_img_flag = CONFIG['update_img_flag']\n",
    "print(\"project_id \",project_id , \" user_name \", user_name,\" user_email \",user_email,\"update_img_flag \",update_img_flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "req_header = {\"X-Project-Id\": project_id, \n",
    "              \"X-Auth-Username\": user_name,\n",
    "              \"X-Auth-Userid\": user_name,\n",
    "              \"X-Auth-Email\": user_email,\n",
    "              \"accept\": \"application/json\"}\n",
    "\n",
    "def tasks_to_perform(image, project_id, model_id, version_id, user_name, user_email, old_model=False): \n",
    "    \"\"\"\n",
    "    If we have old model which was not trained on localhost, we can use old_model = True. \n",
    "    This will delete the init script table. As we are fetching the current model version by current serving,\n",
    "    the old init script will be printed which can be saved for reference by user.\n",
    "    \"\"\"\n",
    "    url = \"http://mosaic-ai-backend:5000\"\n",
    "    global req_header\n",
    "    print(\"########## old_model ####### \",old_model)\n",
    "    if not old_model:\n",
    "        \"\"\"Fetching current to have backup of script displayed\"\"\"\n",
    "        current_serving_resp = get_current_serving_image(url, image, project_id, model_id, version_id)\n",
    "        #print(current_serving_resp)\n",
    "        current_serving_resp_json =  json.loads(current_serving_resp.text)\n",
    "        ''' Commented this to reduce logs in pod as it was just to check serving image updated or not\n",
    "        for key in current_serving_resp_json:\n",
    "            if key in [\"name\", \"id\", \"ml_model_id\", \"current_version\", \"docker_image_url\", \"init_script\"]:\n",
    "                print(key,\" : \",current_serving_resp_json[key])\n",
    "        '''        \n",
    "        resp = update_serving_image(url, image, project_id, model_id, version_id)\n",
    "        #print(resp)\n",
    "\n",
    "    elif old_model:\n",
    "\n",
    "        \"\"\"Fetching current to have backup of script displayed\"\"\"\n",
    "        print(\" In old_model \",old_model)\n",
    "        current_serving_resp = get_current_serving_image(url, image, project_id, model_id, version_id)\n",
    "        #print(current_serving_resp)\n",
    "        current_serving_resp_json =  json.loads(current_serving_resp.text)\n",
    "        ''' Commented this to reduce logs in pod as it was just to check serving image updated or not\n",
    "        for key in current_serving_resp_json:\n",
    "            if key in [\"name\", \"id\", \"ml_model_id\", \"current_version\", \"docker_image_url\", \"init_script\"]:\n",
    "                print(key,\" : \",current_serving_resp_json[key])\n",
    "        ''' \n",
    "        resp = update_old_models(url, image, project_id, model_id, version_id)\n",
    "        #print(resp)\n",
    "        \n",
    "    return resp, current_serving_resp_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger('NER_Training_Service.log')\n",
    "Update_Image_patience = 0\n",
    "if update_img_flag ==True:    \n",
    "    if len(Ensemble_ModelDetails)==0:\n",
    "        model_id = TrainModel['model_id'].split(\"__\")[0]\n",
    "        version_id = TrainModel['model_id'].split(\"__\")[1]\n",
    "        #For updating image\n",
    "        while Update_Image_patience < 10:\n",
    "            try:\n",
    "                print(\"Updating serving image of trained model \")\n",
    "                logger.info(\"Updating serving image of trained model \")\n",
    "                update_response, old_serving_json = tasks_to_perform(static_serving_image, project_id, model_id, version_id, user_name, user_email,old_model=True)\n",
    "                #time.sleep(5)\n",
    "                print(\"Serving image of trained model updated successfully\")\n",
    "                #print(\"Details on add model version with Update_Image_patience \",Update_Image_patience, update_response, old_serving_json)\n",
    "                logger.info(\"Updated the serving image for the model with Update_Image_patience {0} {1} {2}\".format(Update_Image_patience ,update_response, old_serving_json))\n",
    "                break\n",
    "            except BaseException as e:\n",
    "                print(\"Failed to add version to the model with error \", e)\n",
    "                logger.info(\"Failed to add version to the model with error \", e)\n",
    "                Update_Image_patience += 1\n",
    "                print(\"Trying again with next Update_Image_patience - \", Update_Image_patience)\n",
    "                logger.info(\"Trying again with next Update_Image_patience {0}\".format(Update_Image_patience))\n",
    "                time.sleep(1)\n",
    "                pass                \n",
    "    else:\n",
    "        for Each_Model in Ensemble_ModelDetails:\n",
    "            model_id = Each_Model['model_id'].split(\"__\")[0]\n",
    "            version_id = Each_Model['model_id'].split(\"__\")[1]\n",
    "            #For updating image\n",
    "            while Update_Image_patience < 10:\n",
    "                try:\n",
    "                    print(\"Updating serving image of trained model\")\n",
    "                    logger.info(\"Updating serving image of trained model\")\n",
    "                    update_response, old_serving_json = tasks_to_perform(static_serving_image, project_id, model_id, version_id, user_name, user_email,old_model=True)\n",
    "                    #time.sleep(5)\n",
    "                    print(\"Serving image of trained model updated successfully\")\n",
    "                    #print(\"Details on add model version with Update_Image_patience \",Update_Image_patience ,update_response, old_serving_json)\n",
    "                    logger.info(\"Updated the serving image for the model with Update_Image_patience {0} {1} {2}\".format(Update_Image_patience ,update_response, old_serving_json))\n",
    "                    break\n",
    "                except BaseException as e:\n",
    "                    print(\"Failed to add version to the model with error \", e)\n",
    "                    logger.info(\"Failed to add version to the model with error \", e)\n",
    "                    Update_Image_patience += 1\n",
    "                    print(\"Trying again with next Update_Image_patience - \", Update_Image_patience)\n",
    "                    logger.info(\"Trying again with next Update_Image_patience {0}\".format(Update_Image_patience))\n",
    "                    time.sleep(1)\n",
    "                    pass                \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload Model Files to Cloud Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(Ensemble_ModelDetails)==0:\n",
    "    \n",
    "    Folder_name = TrainModel['model_id']\n",
    "    files = os.listdir(os.path.join(CONFIG['MODEL_BASE_LOCATION'], Folder_name))\n",
    "    \n",
    "    print(\"Folder_name - \",Folder_name)\n",
    "    print(\"Files - \",files)\n",
    "    print(\"Files_Path - \",CONFIG['MODEL_BASE_LOCATION']+Folder_name+'/'+files[0])\n",
    "    print(\"List of files - \", os.listdir('/home/mosaic-ai/'))\n",
    "    print(\"Upload files to cloud storage in progress\")\n",
    "    #uploading log file\n",
    "    if OnPremiseFlag!=True:\n",
    "        code, local_path = upload_file_obj_to_cloud_storage(application_config['STORAGE_AT'], open('/home/mosaic-ai/NER_Training_Service.log', 'rb'),\n",
    "                                                            'EEF_models/'+Folder_name+'/NER_Training_Service.log')\n",
    "    else:\n",
    "        if not os.path.exists(os.path.join(CONFIG['TRAINED_MODEL_BASE_PATH'], Folder_name)):\n",
    "            os.makedirs(os.path.join(CONFIG['TRAINED_MODEL_BASE_PATH'], Folder_name))\n",
    "        shutil.copy('/home/mosaic-ai/NER_Training_Service.log',os.path.join(CONFIG['TRAINED_MODEL_BASE_PATH'], Folder_name))\n",
    "        \n",
    "    for i in files:\n",
    "        #file_chk = FileBasePath+Folder_name+'/'+i\n",
    "        file_chk = os.path.join(CONFIG['MODEL_BASE_LOCATION'], Folder_name, i)\n",
    "        if os.path.isfile(file_chk)==True:\n",
    "            start_time = time.time()\n",
    "            print(\"uploading file \", i)\n",
    "            print(\"Uploading file to cloud storage in bucket {} and file {} \".format(Folder_name,i))\n",
    "\n",
    "            if OnPremiseFlag!=True:\n",
    "                #code, local_path = upload_file_obj_to_cloud_storage(application_config['STORAGE_AT'], open(FileBasePath+Folder_name+'/'+i, 'rb'),\n",
    "                code, local_path = upload_file_obj_to_cloud_storage(application_config['STORAGE_AT'], open(os.path.join(CONFIG['MODEL_BASE_LOCATION'], Folder_name, i), 'rb'),\n",
    "                                                                    'EEF_models/'+Folder_name+'/'+i)\n",
    "                print(\"Time require to upload model files in cloud storage is %s seconds ---\" % (time.time() - start_time))\n",
    "            else:\n",
    "                #shutil.copy(FileBasePath+Folder_name+'/'+i, os.path.join(CONFIG['TRAINED_MODEL_BASE_PATH'], Folder_name))\n",
    "                shutil.copy(os.path.join(CONFIG['MODEL_BASE_LOCATION'], Folder_name, i), os.path.join(CONFIG['TRAINED_MODEL_BASE_PATH'], Folder_name))\n",
    "                print(\"model copied to PV storage\")            \n",
    "    print(\"Updating model status\")\n",
    "    update_mosaicml_metadata_info(TrainModel['model_id'],data={'status':app_constants.STATUS_COMPLETED})\n",
    "    print(\"Model status updated to Trained\")\n",
    "    print(\"Upload files to cloud storage is completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(Ensemble_ModelDetails)>0:\n",
    "    for Each_Model in Ensemble_ModelDetails:\n",
    "        if (('NER' in Each_Model['model_type']) and (Each_Model['CCFlag'] == 'False')):\n",
    "            model_path = os.path.join(CONFIG['MODEL_BASE_LOCATION'], Each_Model['model_id'])\n",
    "            print(\"Model path - \",model_path)\n",
    "            print(\"Model type - \",\"NER\")\n",
    "            print(\"In CCFlag False loop\")\n",
    "            model_file_path = model_path+'/valid_char_set.pkl'\n",
    "            model = keras_load_model(model_path+'/'+\"model_complete.h5\")\n",
    "            print(\"Model_file_path...............\",model_file_path)\n",
    "            word2idx = pkl.load(open(model_path+'/'+'word2Idx.pkl','rb'))\n",
    "            label2idx = pkl.load(open(model_path+'/'+'label2Idx.pkl','rb'))\n",
    "            case2idx = pkl.load(open(model_path+'/'+'case2Idx.pkl','rb'))\n",
    "            char2idx = pkl.load(open(model_path+'/'+'char2Idx.pkl','rb'))\n",
    "            valid_char_set = pkl.load(open(model_path+'/'+'valid_char_set.pkl','rb'))\n",
    "            print(\"NER model path - \",model_path)\n",
    "            ml_model_id = Each_Model['model_id'].split(\"__\")[0]\n",
    "            version_id = Each_Model['model_id'].split(\"__\")[1]\n",
    "            model_obj = model\n",
    "            print(\"Updating the model weights in AIL \")\n",
    "            UpdateModel = update_existing_model(ml_model_id, version_id, model_obj, scoring_func = predict_ner, flavour = MLModelFlavours.keras, init_script=None, schema=None, metadata_info=None, input_type='json', target_names=None, datasource_name=None)\n",
    "            print(\"Uploading the model files to cloud storage\")\n",
    "            Folder_name = Each_Model['model_id']\n",
    "            files_list = os.listdir(os.path.join(CONFIG['MODEL_BASE_LOCATION'], Each_Model['model_id']))\n",
    "            print(\"List of files uploading to cloud storage - \", files_list)\n",
    "            print(\"Model id folder - \",Each_Model['model_id'])\n",
    "            #uploading log file\n",
    "            if OnPremiseFlag!=True:\n",
    "                try:\n",
    "                    code, local_path = upload_file_obj_to_cloud_storage(application_config['STORAGE_AT'], open('/home/mosaic-ai/NER_Training_Service.log', 'rb'), 'EEF_models/'+Folder_name+'/NER_Training_Service.log')\n",
    "                except:\n",
    "                    print(\"File NER_Training_Service.log not present in /home/mosaic-ai/ \")\n",
    "                    pass\n",
    "            else:\n",
    "                if not os.path.exists(os.path.join(CONFIG['TRAINED_MODEL_BASE_PATH'], Folder_name)):\n",
    "                    os.makedirs(os.path.join(CONFIG['TRAINED_MODEL_BASE_PATH'],Folder_name))\n",
    "                shutil.copy('/home/mosaic-ai/NER_Training_Service.log', os.path.join(CONFIG['TRAINED_MODEL_BASE_PATH'], Folder_name))\n",
    "                \n",
    "            for i in files_list:\n",
    "                file_chk = os.path.join(CONFIG['MODEL_BASE_LOCATION'], Folder_name, i)\n",
    "                if os.path.isfile(file_chk)==True:\n",
    "                    start_time = time.time()\n",
    "                    #print(\"Uploading file - \", i)\n",
    "                    print(\"Uploading file to cloud storage in bucket {} and file {} \".format(Folder_name,i))\n",
    "                    if OnPremiseFlag!=True:\n",
    "                        code, local_path = upload_file_obj_to_cloud_storage(application_config['STORAGE_AT'], open(os.path.join(CONFIG['MODEL_BASE_LOCATION'], Folder_name, i), 'rb'), 'EEF_models/'+Folder_name+'/'+i)\n",
    "                        print(\"Time require to upload model files in cloud storage is %s seconds.\" % (time.time() - start_time))\n",
    "                    else:\n",
    "                        shutil.copy(os.path.join(CONFIG['MODEL_BASE_LOCATION'], Folder_name, i), os.path.join(CONFIG['TRAINED_MODEL_BASE_PATH'], Folder_name))\n",
    "                        print(\"Model copied to PV storage\")\n",
    "                        \n",
    "            print(\"Upload files to cloud storage is completed\")\n",
    "            print(\"Updating model status\")\n",
    "            update_mosaicml_metadata_info(Each_Model['model_id'],data={'status':app_constants.STATUS_COMPLETED})\n",
    "            print(\"Model status updated to Trained\")\n",
    "            \n",
    "                \n",
    "        if (('NER' in Each_Model['model_type']) and (Each_Model['CCFlag'] == 'True')):\n",
    "            model_path = os.path.join(CONFIG['MODEL_BASE_LOCATION'], Each_Model['model_id'])\n",
    "            print(\"Model path - \",model_path)\n",
    "            print(\"Model type\",\"NER\")\n",
    "            print(\"CCFlag True loop\")\n",
    "            word2idx = pkl.load(open(model_path+'/'+'word2Idx.pkl','rb'))\n",
    "            label2idx = pkl.load(open(model_path+'/'+'label2Idx.pkl','rb'))\n",
    "            case2idx = pkl.load(open(model_path+'/'+'case2Idx.pkl','rb'))\n",
    "            char2idx = pkl.load(open(model_path+'/'+'char2Idx.pkl','rb'))\n",
    "            valid_char_set = pkl.load(open(model_path+'/'+'valid_char_set.pkl','rb'))\n",
    "            svc_clf_file_name = '%s_content_classification_model.pkl' % Each_Model['model_id']\n",
    "            tf_idf_model_file_name = '%s_content_classification_tf_idf_model.pkl' % Each_Model['model_id']\n",
    "            svc_clf = pkl.load(open(model_path+'/'+svc_clf_file_name,'rb'))\n",
    "            tf_idf_model = pkl.load(open(model_path+'/'+tf_idf_model_file_name,'rb'))\n",
    "            print(\"svc_clf - \",model_path+'/'+svc_clf_file_name)\n",
    "            print(\"tf_idf_model - \",model_path+'/'+tf_idf_model_file_name)\n",
    "            ml_model_id = Each_Model['model_id'].split(\"__\")[0]\n",
    "            version_id = Each_Model['model_id'].split(\"__\")[1]\n",
    "            model = keras_load_model(model_path+'/'+\"model_complete.h5\")\n",
    "            model_obj = model\n",
    "            print(\"Updating the model weights in AIL \")\n",
    "            UpdateModel = update_existing_model(ml_model_id, version_id, model_obj, scoring_func = predict_ner_CC, flavour = MLModelFlavours.keras, init_script=None, schema=None, metadata_info=None, input_type='json', target_names=None, datasource_name=None)\n",
    "            print(\"Uploading the model files to cloud storage\")\n",
    "            Folder_name = Each_Model['model_id']\n",
    "            files_list = os.listdir(os.path.join(CONFIG['MODEL_BASE_LOCATION'], Each_Model['model_id']))\n",
    "            print(\"List of files uploading to cloud storage - \", files_list)\n",
    "            print(\"Model id folder - \",Each_Model['model_id'])\n",
    "            #uploading log file\n",
    "            if OnPremiseFlag!=True:                \n",
    "                try:\n",
    "                    code, local_path = upload_file_obj_to_cloud_storage(application_config['STORAGE_AT'], open('/home/mosaic-ai/NER_Training_Service.log', 'rb'), 'EEF_models/'+Folder_name+'/NER_Training_Service.log')\n",
    "                    upload_file('/home/mosaic-ai/NER_Training_Service.log',bucket.name,'EEF_models/'+Folder_name)\n",
    "                except:\n",
    "                    print(\"File NER_Training_Service.log not present in /home/mosaic-ai/ \")\n",
    "                    pass\n",
    "            else:\n",
    "                if not os.path.exists(os.path.join(CONFIG['TRAINED_MODEL_BASE_PATH'], Folder_name)):\n",
    "                    os.makedirs(os.path.join(CONFIG['TRAINED_MODEL_BASE_PATH'], Folder_name))\n",
    "                shutil.copy('/home/mosaic-ai/NER_Training_Service.log', os.path.join(CONFIG['TRAINED_MODEL_BASE_PATH'], Folder_name))            \n",
    "            \n",
    "            for i in files_list:\n",
    "                file_chk = os.path.join(CONFIG['MODEL_BASE_LOCATION'], Folder_name, i)\n",
    "                if os.path.isfile(file_chk)==True:\n",
    "                    start_time = time.time()\n",
    "                    #print(\"Uploading file - \",i)\n",
    "                    print(\"Uploading file to cloud storage in bucket {} and file {} \".format(Folder_name,i))\n",
    "                    if OnPremiseFlag!=True:\n",
    "                        code, local_path = upload_file_obj_to_cloud_storage(application_config['STORAGE_AT'], open(os.path.join(CONFIG['MODEL_BASE_LOCATION'], Folder_name, i), 'rb'), 'EEF_models/'+Folder_name+'/'+i)\n",
    "                        print(\"Time require to upload model files in cloud storage bucket is %s seconds.\" % (time.time() - start_time))\n",
    "                    else:\n",
    "                        shutil.copy(os.path.join(CONFIG['MODEL_BASE_LOCATION'], Folder_name, i), os.path.join(CONFIG['TRAINED_MODEL_BASE_PATH'], Folder_name))\n",
    "                        print(\"Model copied to PV storage\")\n",
    "\n",
    "            print(\"Upload files to cloud storage is completed\")\n",
    "            print(\"updating model status     \")\n",
    "            update_mosaicml_metadata_info(Each_Model['model_id'],data={'status':app_constants.STATUS_COMPLETED})\n",
    "            print(\"model status updated to Trained    \")\n",
    "                \n",
    "                \n",
    "        if 'DC' in Each_Model['model_type']:\n",
    "            model_path = os.path.join(CONFIG['MODEL_BASE_LOCATION'], Each_Model['model_id'])\n",
    "            print(\"Model path - \",model_path)\n",
    "            print(\"Model type - \", \"DC\")\n",
    "            svc_clf_file_name = '%s_document_classification_svc_model.pkl' % Each_Model['model_id']\n",
    "            tf_idf_model_file_name = '%s_document_classification_tf_idf_model.pkl' % Each_Model['model_id']\n",
    "            input_data_file_name = '%s_document_classification_input_data.pkl' % Each_Model['model_id']\n",
    "            svc_clf = pkl.load(open(model_path+'/'+svc_clf_file_name,'rb'))\n",
    "            tf_idf_model = pkl.load(open(model_path+'/'+tf_idf_model_file_name,'rb'))\n",
    "            dc_input_data = pkl.load(open(model_path+'/'+input_data_file_name,'rb'))\n",
    "            print(\"svc_clf - \",model_path+'/'+svc_clf_file_name)\n",
    "            print(\"tf_idf_model - \",model_path+'/'+tf_idf_model_file_name)\n",
    "            print(\"dc_input_data - \",model_path+'/'+input_data_file_name)\n",
    "            model = tf_idf_model\n",
    "            ml_model_id = Each_Model['model_id'].split(\"__\")[0]\n",
    "            version_id = Each_Model['model_id'].split(\"__\")[1]\n",
    "            model_obj = model\n",
    "            print(\"Updating the model weights in AIL \")\n",
    "            UpdateModel = update_existing_model(ml_model_id, version_id, model_obj, scoring_func = predict_Doc_classification, flavour = MLModelFlavours.sklearn, init_script=None, schema=None, metadata_info=None, input_type='json', target_names=None, datasource_name=None)\n",
    "            print(\"Uploading the model files to cloud storage\")\n",
    "            Folder_name = Each_Model['model_id']\n",
    "            files_list = os.listdir(os.path.join(CONFIG['MODEL_BASE_LOCATION'], Each_Model['model_id']))\n",
    "            print(\"List of files uploading to cloud storage - \", files_list)\n",
    "            print(\"Model id folder - \",Each_Model['model_id'])\n",
    "            #uploading log file\n",
    "            if OnPremiseFlag!=True:\n",
    "                try:\n",
    "                    code, local_path = upload_file_obj_to_cloud_storage(application_config['STORAGE_AT'], open('/home/mosaic-ai/NER_Training_Service.log', 'rb'), 'EEF_models/'+Folder_name+'/NER_Training_Service.log')\n",
    "                except:\n",
    "                    print(\"File NER_Training_Service.log not present in /home/mosaic-ai/ \")\n",
    "                    pass\n",
    "            else:\n",
    "                if not os.path.exists(os.path.join(CONFIG['TRAINED_MODEL_BASE_PATH'], Folder_name)):\n",
    "                    os.makedirs(os.path.join(CONFIG['TRAINED_MODEL_BASE_PATH'], Folder_name))\n",
    "                shutil.copy('/home/mosaic-ai/NER_Training_Service.log', os.path.join(CONFIG['TRAINED_MODEL_BASE_PATH'], Folder_name))            \n",
    "            \n",
    "            for i in files_list:\n",
    "                file_chk = os.path.join(CONFIG['MODEL_BASE_LOCATION'], Folder_name, i)\n",
    "                if os.path.isfile(file_chk)==True:\n",
    "                    start_time = time.time()\n",
    "                    #print(\"Uploading file \",i)\n",
    "                    print(\"Uploading file to cloud storage in bucket {} and file {} \".format(Folder_name,i))\n",
    "                    if OnPremiseFlag!=True:\n",
    "                        code, local_path = upload_file_obj_to_cloud_storage(application_config['STORAGE_AT'], open(os.path.join(CONFIG['MODEL_BASE_LOCATION'], Folder_name, i), 'rb'), 'EEF_models/'+Folder_name+'/'+i)\n",
    "                        print(\"---Time require to upload model files in cloud storage in bucket is %s seconds ---\" % (time.time() - start_time))\n",
    "                    else:\n",
    "                        shutil.copy(os.path.join(CONFIG['MODEL_BASE_LOCATION'], Folder_name, i), os.path.join(CONFIG['TRAINED_MODEL_BASE_PATH'], Folder_name))\n",
    "                        print(\"Model copied to /data PV\")    \n",
    "\n",
    "            print(\"Upload files to cloud storage is completed\")\n",
    "            print(\"Updating model status\")\n",
    "            update_mosaicml_metadata_info(Each_Model['model_id'],data={'status':app_constants.STATUS_COMPLETED})\n",
    "            print(\"Model status updated to Trained\")\n",
    "\n",
    "        if 'IC' in Each_Model['model_type']:\n",
    "            print(\"In IC ensemble loop\")\n",
    "            model_path = os.path.join(CONFIG['MODEL_BASE_LOCATION'], Each_Model['model_id'])\n",
    "            print(\"Model path - \",model_path)\n",
    "            model_file_path = model_path+'/model_complete.h5'\n",
    "            print(\"Model_file_path...............\",model_file_path)\n",
    "            print(\"Image_classes_encoder file check - \",os.listdir(model_path))\n",
    "            class_encoder = pkl.load(open(model_path+'/'+'image_classes_encoder.pkl','rb'))\n",
    "            print(\"Class_encoder - \",model_path+'/'+'image_classes_encoder.pkl')\n",
    "            model = keras_load_model(model_path+'/'+\"model_complete.h5\")\n",
    "            print(\"Loaded IC model\")\n",
    "            ml_model_id = Each_Model['model_id'].split(\"__\")[0]\n",
    "            version_id = Each_Model['model_id'].split(\"__\")[1]            \n",
    "            model_obj = model\n",
    "            print(\"Updating the model weights in AIL \")\n",
    "            UpdateModel = update_existing_model(ml_model_id, version_id, model_obj, scoring_func = predict_IC, flavour = MLModelFlavours.keras, init_script=None, schema=None, metadata_info=None, input_type='json', target_names=None, datasource_name=None)\n",
    "            print(\"Image Classification UpdateModel Details..\", UpdateModel)\n",
    "            print(\"Uploading the model files to cloud storage\")\n",
    "            Folder_name = Each_Model['model_id']\n",
    "            files_list = os.listdir(os.path.join(CONFIG['MODEL_BASE_LOCATION'], Each_Model['model_id']))\n",
    "            print(\"List of files uploading to cloud storage - \", files_list)\n",
    "            print(\"Model id folder - \",Each_Model['model_id'])\n",
    "            #uploading log file\n",
    "            if OnPremiseFlag!=True:\n",
    "                try:\n",
    "                    code, local_path = upload_file_obj_to_cloud_storage(application_config['STORAGE_AT'], open('/home/mosaic-ai/NER_Training_Service.log', 'rb'), 'EEF_models/'+Folder_name+'/NER_Training_Service.log')\n",
    "                except:\n",
    "                    print(\"File NER_Training_Service.log not present in /home/mosaic-ai/ \")\n",
    "                    pass\n",
    "            else:\n",
    "                if not os.path.exists(os.path.join(CONFIG['TRAINED_MODEL_BASE_PATH'], Folder_name)):\n",
    "                    os.makedirs(os.path.join(CONFIG['TRAINED_MODEL_BASE_PATH'], Folder_name))\n",
    "                shutil.copy('/home/mosaic-ai/NER_Training_Service.log', os.path.join(CONFIG['TRAINED_MODEL_BASE_PATH'], Folder_name))            \n",
    "            \n",
    "            for i in files_list:\n",
    "                file_chk = os.path.join(CONFIG['MODEL_BASE_LOCATION'], Folder_name, i)\n",
    "                if os.path.isfile(file_chk)==True:\n",
    "                    start_time = time.time()\n",
    "                    #print(\"Uploading file \",i)\n",
    "                    print(\"Uploading file to cloud storage in bucket {} and file {} \".format(Folder_name,i))\n",
    "                    if OnPremiseFlag!=True:\n",
    "                        code, local_path = upload_file_obj_to_cloud_storage(application_config['STORAGE_AT'], open(os.path.join(CONFIG['MODEL_BASE_LOCATION'], Folder_name, i), 'rb'), 'EEF_models/'+Folder_name+'/'+i)\n",
    "                        print(\"Time require to upload model files in cloud storage bucket is %s seconds.\" % (time.time() - start_time))\n",
    "                    else:\n",
    "                        shutil.copy(os.path.join(CONFIG['MODEL_BASE_LOCATION'], Folder_name, i), os.path.join(CONFIG['TRAINED_MODEL_BASE_PATH'], Folder_name))\n",
    "                        print(\"Model copied to /data PV\")\n",
    "\n",
    "            print(\"Upload files to cloud storage is completed\")\n",
    "            print(\"Updating model status\")\n",
    "            update_mosaicml_metadata_info(Each_Model['model_id'],data={'status':app_constants.STATUS_COMPLETED})\n",
    "            print(\"Model status updated to Trained\")\n",
    "            \n",
    "        if 'Object Identification' in Each_Model['model_type']:\n",
    "            model_path = os.path.join(CONFIG['MODEL_BASE_LOCATION'], Each_Model['model_id'])\n",
    "            print(\"Model path - \",model_path)\n",
    "            model_file_path = model_path+'/frozen_inference_graph.pb'\n",
    "            print(\"Model_file_path - \",model_file_path)\n",
    "            zoning_file_label_path = model_path+'/'+'label_map.pbtxt'\n",
    "            label_map = label_map_util.load_labelmap(zoning_file_label_path)\n",
    "            num_classes = len(label_map.item)\n",
    "            categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=num_classes, use_display_name=True)\n",
    "            category_index = label_map_util.create_category_index(categories)\n",
    "            img_rescale_flag = Each_Model['img_rescale_flag']\n",
    "            OI_MultiCropFlag = Each_Model['OI_MultiCropFlag']\n",
    "            zone_config = Each_Model['zone_config']            \n",
    "            print(\"Zoning_file_label_path - \",zoning_file_label_path)\n",
    "            print(\"Category_index - \",category_index)\n",
    "            print(\"Dynamic image rescale flag - \",img_rescale_flag)\n",
    "            print(\"OI_MultiCropFlag value \", OI_MultiCropFlag)\n",
    "            print(\"zone_config value \", zone_config)            \n",
    "            model = get_zoning_model(model_file_path)\n",
    "            print(\"Loaded Object Identification model\")\n",
    "            ml_model_id = Each_Model['model_id'].split(\"__\")[0]\n",
    "            version_id = Each_Model['model_id'].split(\"__\")[1]            \n",
    "            model_obj = model\n",
    "            print(\"Updating the model weights in AIL \")\n",
    "            UpdateModel = update_existing_model(ml_model_id, version_id, model_obj, scoring_func = predict_zoning_inference_new, flavour = MLModelFlavours.tensorflow, init_script=None, schema=None, metadata_info=None, input_type='json', target_names=None, datasource_name=None)\n",
    "            print(\"Object Identification UpdateModel Details..\", UpdateModel)\n",
    "            print(\"Uploading the model files to cloud storage\")\n",
    "            Folder_name = Each_Model['model_id']\n",
    "            files_list = os.listdir(os.path.join(CONFIG['MODEL_BASE_LOCATION'], Each_Model['model_id']))\n",
    "            print(\"List of files uploading to cloud storage - \", files_list)\n",
    "            print(\"Model id folder - \",Each_Model['model_id'])\n",
    "            #uploading log file\n",
    "\n",
    "            if OnPremiseFlag!=True:\n",
    "                try:\n",
    "                    code, local_path = upload_file_obj_to_cloud_storage(application_config['STORAGE_AT'], open('/home/mosaic-ai/NER_Training_Service.log', 'rb'), 'EEF_models/'+Folder_name+'/NER_Training_Service.log')\n",
    "                except:\n",
    "                    print(\"File NER_Training_Service.log not present in /home/mosaic-ai/ \")\n",
    "                    pass\n",
    "            else:\n",
    "                if not os.path.exists(os.path.join(CONFIG['TRAINED_MODEL_BASE_PATH'], Folder_name)):\n",
    "                    os.makedirs(os.path.join(CONFIG['TRAINED_MODEL_BASE_PATH'], Folder_name))\n",
    "                shutil.copy('/home/mosaic-ai/NER_Training_Service.log', os.path.join(CONFIG['TRAINED_MODEL_BASE_PATH'], Folder_name))            \n",
    "                \n",
    "            for i in files_list:\n",
    "                file_chk = os.path.join(CONFIG['MODEL_BASE_LOCATION'], Folder_name, i)\n",
    "                if os.path.isfile(file_chk)==True:\n",
    "                    start_time = time.time()\n",
    "                    #print(\"Uploading file \",i)\n",
    "                    print(\"Uploading file to cloud storage in bucket {} and file {} \".format(Folder_name,i))\n",
    "                    if OnPremiseFlag!=True:\n",
    "                        code, local_path = upload_file_obj_to_cloud_storage(application_config['STORAGE_AT'], open(os.path.join(CONFIG['MODEL_BASE_LOCATION'], Folder_name, i), 'rb'), 'EEF_models/'+Folder_name+'/'+i)\n",
    "                        print(\"Time require to upload model files in cloud storage bucket is %s seconds\" % (time.time() - start_time))\n",
    "                    else:\n",
    "                        shutil.copy(os.path.join(CONFIG['MODEL_BASE_LOCATION'], Folder_name, i), os.path.join(CONFIG['TRAINED_MODEL_BASE_PATH'], Folder_name))\n",
    "                        print(\"Model copied to PV storage\")\n",
    "\n",
    "            print(\"Upload files to cloud storage is completed\")              \n",
    "            print(\"Updating model status\")\n",
    "            update_mosaicml_metadata_info(Each_Model['model_id'],data={'status':app_constants.STATUS_COMPLETED})\n",
    "            print(\"Model status updated to Trained\")            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Below 2 cells added by pranav for testing AIL model storage, will be deleted after testing\n",
    "#m2 = load_model(ml_model_id=\"002a4aff-7e16-4c3f-9e2e-00e1b863d901\",version_id=\"02d70d85-f025-49e2-bd0e-5c4103a7c738\")  # NER\n",
    "\n",
    "#m2 = load_model(ml_model_id=\"0091d311-09d9-43e8-ab1e-f29ba2718583\",version_id=\"990fc9d5-e657-41be-8806-be4d871e1ea1\")   #OI\n",
    "\n",
    "#m2 = load_model(ml_model_id=\"0091d311-09d9-43e8-ab1e-f29ba2718583\",version_id=\"990fc9d5-e657-41be-8806-be4d871e1ea1\")   #IC\n",
    "\n",
    "#m2 = load_model(ml_model_id=\"cc404c42-6747-4945-9696-adb096641661\",version_id=\"23fca424-be6a-446e-b463-681ca3047569\")   #DC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_type' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-01c9684d6802>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0;34m'NER'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_type\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mCCFlag\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'False'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m      \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"NER and CCFlag False\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m      \u001b[0;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m      \u001b[0mreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m      req.json={\"payload\":{\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_type' is not defined"
     ]
    }
   ],
   "source": [
    "'''\n",
    "#code for debugging inference of ner\n",
    "if 'NER' in model_type and CCFlag == 'False':\n",
    "     print(\"NER and CCFlag False\")\n",
    "     import requests\n",
    "     req = requests.Request()\n",
    "     req.json={\"payload\":{\n",
    "\t\"entity_order_map\": {\n",
    "\t\t\"Invoice_number\": 1,\n",
    "\t\t\"amount_due\": 5,\n",
    "\t\t\"contract_number\": 6,\n",
    "\t\t\"customer_id\": 2,\n",
    "\t\t\"job_number\": 4,\n",
    "\t\t\"purchase_order\": 3\n",
    "\t},\n",
    "\t\"file_id\": \"a517f64b-7f6d-45e5-a6ae-34a3289a5d7e\",\n",
    "\t\"knowledge_graph\": [],\n",
    "\t\"language\": \"english\",\n",
    "\t\"mode_type\": \"mode_1\",\n",
    "\t\"model_id\": \"af81d63d-599d-49ba-a9ed-4fd9d2fc898a\",\n",
    "\t\"prediction_format\": 1,\n",
    "\t\"root_level_ents\": [\"Invoice_number\", \"customer_id\", \"purchase_order\", \"job_number\", \"amount_due\", \"contract_number\"],\n",
    "\t\"text\": {\n",
    "\t\t\"1\": {\n",
    "\t\t\t\"1\": {\n",
    "\t\t\t\t\"sentence\": \"START?\",\n",
    "\t\t\t\t\"unique_id\": \"e6750f87-c9e6-470f-a76b-2c0cd39e8c24\"\n",
    "\t\t\t},\n",
    "\t\t\t\"2\": {\n",
    "\t\t\t\t\"sentence\": \"FINISH\\nPlease send remittance to:\\nStart 2 Finish\\n30 West Third Street, Suite 4M\\nCincinnati, Ohio 45202\\nINVOICE\\n10108\\nCONSTRUCTION & FACILITIES MANAGEMENT SOLUTIONS\\nPage 1 of 1\\n30 West Third Street, Ste 4M\\nCincinnati, OH 45202\\nPURCHASE ORDER\\nPAYMENT TERMS\\nAMOUNT DUE\\n3012708-OP-4014402000\\nNet 90\\n$9,656.00\\nINVOICE DATE\\nCUSTOMER ID\\nJOB Procter & Gamble GO\\nBILL Jones Lang LaSalle Services\\nTO: LTD.\\nC/O Jones Lang Lasalle\\nAccounting\\nP.O.\",\n",
    "\t\t\t\t\"unique_id\": \"7245f39d-b016-4b34-ba3e-ba0681515900\"\n",
    "\t\t\t},\n",
    "\t\t\t\"3\": {\n",
    "\t\t\t\t\"sentence\": \"Box 5126\\nCincinnati, OH 45201-5126\\n1/08/2018\\n350008\\nSITE: 301 East 6th Street\\nCincinnati, OH 45202\\nJOB NUMBER\\n6500980008\\nJOB LOCATION:\\nCONTRACT NUMBER:\\nCONTRACT DESCRIPTION:\\nP&G - HO\\n50080008\\nStationary\\nJohn Rail\\nQUANTITY\\nDESCRIPTION\\nAMOUNT\",\n",
    "\t\t\t\t\"unique_id\": \"bf680b73-97ee-4e2d-832b-a308100cd01b\"\n",
    "\t\t\t}\n",
    "\t\t},\n",
    "\t\t\"2\": {\n",
    "\t\t\t\"1\": {\n",
    "\t\t\t\t\"sentence\": \"12\\nStationary\\n8,685.00\\nTOTAL SALES:\\n8,685.00\\nSALES TAX:\\n971.00\\nINVOICE AMOUNT:\\n$9,656.00\\nADDITIONAL BILLING INFORMATION:\\nProvide labor, material and equipment to replace (1) set of stationary items.\",\n",
    "\t\t\t\t\"unique_id\": \"6471b802-075c-4e1f-9ebf-528af1a75993\"\n",
    "\t\t\t}\n",
    "\t\t}\n",
    "\t}\n",
    "}}\n",
    "     result = predict_ner(model,req)\n",
    "     print(\"result\",result)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'case2idx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-8ede55847b7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#m2[1].__globals__['svc_clf']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#m2[1].__globals__['tf_idf_data']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mm2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__globals__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'case2idx'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'case2idx'"
     ]
    }
   ],
   "source": [
    "#m2\n",
    "#temp = m2[1].__globals__['dc_input_data']\n",
    "#type(temp)\n",
    "#m2[1].__globals__['svc_clf']\n",
    "#m2[1].__globals__['tf_idf_data']\n",
    "#m2[1].__globals__['case2idx']"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
